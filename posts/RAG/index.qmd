---
title: "Retrieval-Augmented Generation (RAG)"
subtitle: "RAG for Knowledge-Intensive NLP"
author: "Mulimbika Makina"
date: 2026-02-04
format:
  html:
    toc: true
    toc-depth: 2
    number-sections: true
    code-fold: true         # ← Makes all code blocks collapsible
    code-summary: "View Code"  # ← Label for collapsed blocks
    df-print: paged
categories: [Retrieval-Augmented Generation, Large Language Models (LLMs), Document Ingestion, Document Parsing, Query Processing, Reranking, Vector Databases]

---


![](rag.jpg)


# Overview
Large Language Models (LLMs) are trained using huge amounts of data collected from the internet, such as websites, books, news articles, and online conversations. This broad training allows them to understand language well and handle many general tasks. However, when it comes to specialized or professional use cases, these models often fall short because the required domain knowledge is usually private, sensitive, or not publicly available, and therefore not included in their training data.

Retrieval-Augmented Generation (RAG) helps solve this problem by allowing LLMs to access external knowledge when answering questions. In a RAG setup, *a user asks a question*, *the system searches a private knowledge base for relevant information*, *combines that information with the question*, and *then uses the LLM to produce a more accurate and context-aware response*.

**This project focuses on engineering a scalable, modular RAG system with measurable performance and reliability guarantees.**

***The main areas of focus include:***

- Parser selection and ingestion quality

- Retrieval strategy design (dense, sparse, hybrid)

- Prompt and context engineering

- Reranking and query transformation

- User-facing answer quality and grounding

***Expected Outcomes***

- Reduced hallucinations through grounded generation

- Improved answer relevance for ambiguous queries

- Robust handling of noisy and heterogeneous documents

- Explainable and source-aware responses

# Problem Definition

Organizations store critical knowledge in large collections of unstructured documents such as PDFs, reports, manuals, and internal policies. While Large Language Models (LLMs) provide strong language understanding capabilities, they cannot reliably answer questions over private or evolving document collections without hallucinating or losing traceability.

The core challenge is to design a Retrieval-Augmented Generation (RAG) system that:

`Grounds LLM outputs in retrieved evidence`, `Scales to large document collections`, `Maintains low latency and cost`, And `can be evaluated and improved systematically`.

# Document Ingestion & Parsing

For humans, the process of reading a document is intuitive and largely the same regardless of the content. When we look at a page, our eyes capture individual characters, which our brains then organize into meaningful structures such as paragraphs, tables, and charts. From there, we understand, interpret, and remember the information. Computers, in contrast, perceive information only as binary data, without any inherent understanding of structure or meaning.

Large Language Models (LLMs) are highly effective at processing serialized text. However, to enable them to work reliably with unstructured or untagged documents, an intermediate parsing step is required. Document parsers transforms scattered characters into coherent, structured text, preserving the logical organization of the original document so that the LLM can interpret the content accurately.

## Document Parsers for RAG (Commonly used)

| Framework | Type | Best For |
|-----------|------|----------|
| **Docling** | Open-source (IBM) | Scientific documents, tables |
| **Unstructured** | Open-source | General purpose, fast processing |
| **LlamaParse** | Cloud service | Complex layouts, multimodal |
| **Vectorize.io** | Cloud platform | End-to-end RAG pipelines |

### Docling [Link](https://www.docling.ai/)

IBM's open-source document parser with advanced table recognition.

**Features:**
- PDF, DOCX, PPTX, HTML, image parsing
- Multiple OCR engines (RapidOCR, EasyOCR, Tesseract)
- TableFormer for table structure recognition
- VLM (Vision-Language Model) pipeline support
- LangChain integration via DoclingLoader

::: {.callout-tip title="Implementation Snippet" collapse=true}
```{python}
#| eval: false
from docling.document_converter import DocumentConverter

converter = DocumentConverter()
result = converter.convert("document.pdf")
markdown = result.document.export_to_markdown()
```
:::


### Unstructured [Link](https://unstructured.io/)

Fast, general-purpose document processing library.

**Features:**
- Auto-detection with `partition()` function
- Multiple strategies: fast, hi_res, ocr_only
- Element type filtering (Title, Table, NarrativeText, etc.)
- Built-in chunking strategies
- LangChain UnstructuredLoader integration

::: {.callout-tip title="Implementation Snippet" collapse=true}
```{python}
#| eval: false
from unstructured.partition.auto import partition

elements = partition(filename="document.pdf", strategy="hi_res")
tables = [e for e in elements if e.category == "Table"]
```
:::

### LlamaParse [Link](https://developers.llamaindex.ai/python/cloud/llamaparse/)

LlamaIndex's GenAI-native cloud parser for complex documents.

**Features:**
- Presets: fast, balanced, premium, scientific, invoice, slides
- Multimodal extraction (charts, screenshots, OCR)
- Structured output with JSON schemas
- Custom parsing instructions
- Async batch processing

::: {.callout-tip title="Implementation Snippet" collapse=true}
```{python}
#| eval: false
from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex

parser = LlamaParse(result_type="markdown", preset="scientific")
documents = parser.load_data("document.pdf")

index = VectorStoreIndex.from_documents(documents)
query_engine = index.as_query_engine()
```
:::

**Pricing:** Free tier 1,000 pages/day, Paid $0.003/page

### Vectorize.io [Link](https://vectorize.io/)

All-in-one RAG platform with built-in vector database.

**Features:**
- Iris AI-powered extraction (100+ languages)
- Built-in RAG Pipeline Builder (free vector DB)
- Source connectors: File Upload, S3, Google Drive, SharePoint
- Semantic search with re-ranking
- Deep Research for AI-generated reports

::: {.callout-tip title="Implementation Snippet" collapse=true}
```{python}
#| eval: false
import vectorize_client as v

# Create pipeline
pipeline = pipelines_api.create_pipeline(
    org_id,
    v.CreatePipelineRequest(
        name="my_pipeline",
        extraction=v.ExtractionConfig(strategy="IRIS"),
        embedder=v.EmbedderConfig(provider="OPENAI"),
        vector_database=v.VectorDatabaseConfig(type="VECTORIZE")
    )
)

# Retrieve documents
response = pipelines_api.retrieve_documents(
    org_id, pipeline_id,
    v.RetrieveDocumentsRequest(question="your query", rerank=True)
)
```
:::

**Pricing:** Free tier 1,000 pages/day for Iris extraction

## Framework Comparison

| Feature | Docling | Unstructured | LlamaParse | Vectorize.io |
|---------|---------|--------------|------------|--------------|
| **Cost** | Free | Free | 1K pages/day free | 1K pages/day free |
| **OCR** | Multiple engines | Tesseract | Built-in | AI-powered |
| **Tables** | TableFormer | Built-in | AI-powered | AI-powered |
| **Speed** | Medium | Fast | Fast | Fast |
| **Built-in Vector DB** | No | No | No | Yes (free) |
| **Deep Research** | No | No | No | Yes |

---

# Chunking Strategies

Chunking is the process of breaking large documents into smaller, manageable pieces of text that can be efficiently stored, indexed, and retrieved. Its primary purpose is to preserve meaningful context while ensuring that information fits within the context limits of embedding models and large language models (LLMs). Effective chunking improves retrieval precision, reduces noise, and helps LLMs generate more accurate and grounded responses.

In a Retrieval-Augmented Generation (RAG) pipeline, chunking occurs during the data ingestion and indexing stage, before embeddings are created and stored in a vector database. The choice of chunking strategy directly influences what information is retrieved at query time and, ultimately, the quality of the generated answers. The table below outlines common chunking strategies used in RAG systems, along with their key advantages and trade-offs.



| **Chunking Strategy**                 | **Brief Description**                                                                                                   | **Advantages**                                                                       | **Disadvantages**                                                                          |
| ------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------ |
| **Fixed-size chunking**               | Splits text into chunks of a predefined token or character length, often with optional overlap.                         | Simple to implement; fast; predictable chunk sizes; works well for homogeneous text. | Ignores semantic boundaries; may split sentences or ideas; can reduce retrieval relevance. |
| **Semantic chunking**                 | Groups text based on semantic similarity, often at sentence or paragraph level.                                         | Preserves meaning; improves retrieval accuracy; better context coherence.            | Computationally expensive; depends on embeddings quality; less predictable chunk sizes.    |
| **Recursive chunking**                | Splits text hierarchically, starting from larger sections and recursively breaking them down until size limits are met. | Balances structure and size constraints; flexible; widely used in RAG frameworks.    | More complex logic; still may split meaningful units if limits are strict.                 |
| **Document structure-based chunking** | Uses inherent document structure (titles, headings, sections) to create chunks.                                         | Maintains logical flow; ideal for well-formatted documents; high interpretability.   | Fails on poorly structured documents; relies on accurate parsing.                          |
| **LLM-based chunking**                | Uses an LLM to dynamically decide how to split text into meaningful chunks.                                             | Highly adaptive; preserves semantics and intent; handles complex documents well.     | Slow; costly; less deterministic; harder to debug or reproduce results.                    |

---

### Still under development...

<!-- ---

# Embeddings & Vector Representations

*Purpose:* Transform text chunks into numerical vector representations for semantic similarity search.

## Dense Embeddings for Semantic Search

*Purpose:* Generate dense vector embeddings that capture semantic meaning.

## Sparse Embeddings and Keyword Signals

*Purpose:* Incorporate keyword-based signals to complement dense embeddings.

## Advanced Sparse Embedding Techniques

*Purpose:* Apply advanced sparse representations for improved hybrid retrieval.

---

# Vector Stores & Indexing

*Purpose:* Store and index embeddings efficiently to enable fast and scalable similarity search.

## Vector Store Fundamentals

*Purpose:* Introduce core concepts behind vector storage and indexing.

## Index Construction and Similarity Search

*Purpose:* Build and query vector indexes for nearest-neighbor retrieval.

## Index Persistence and Scaling

*Purpose:* Persist vector indexes and scale retrieval systems for production use.

---

# Retrieval Strategies

*Purpose:* Retrieve the most relevant document chunks in response to user queries.

## Basic Similarity-Based Retrieval

*Purpose:* Retrieve top-K chunks using embedding similarity.

## Fusion Retrieval (Dense + Sparse)

*Purpose:* Combine dense and sparse retrieval methods to improve recall and robustness.

## Adaptive Retrieval Strategies

*Purpose:* Dynamically adjust retrieval behavior based on query complexity.

## External Index Retrievers

*Purpose:* Integrate external vector or search indexes into the retrieval pipeline.

---

# Query & Context Optimization

*Purpose:* Improve retrieval effectiveness by refining queries and managing context windows.

## Query Transformations

*Purpose:* Rewrite and expand user queries to improve retrieval performance.

## Context Enrichment with Sliding Windows

*Purpose:* Enrich retrieved context by aggregating adjacent chunks intelligently.

---

# Reranking for Retrieval Quality

*Purpose:* Improve retrieval precision by reordering retrieved results using stronger models.

## Reranking Fundamentals

*Purpose:* Introduce reranking concepts and their role in retrieval quality.

## Cross-Encoder Reranking Pipelines

*Purpose:* Apply cross-encoder models to rerank retrieved chunks for maximum relevance.

---

# End-to-End RAG Pipelines

*Purpose:* Integrate ingestion, retrieval, and generation into a complete RAG system.

## Building a Complete RAG Pipeline

*Purpose:* Assemble all components into a functional end-to-end RAG workflow.

## Modular vs Monolithic RAG Design

*Purpose:* Compare design approaches for building maintainable and scalable RAG systems.

---

# Advanced Retrieval Techniques

*Purpose:* Explore advanced methods to improve retrieval under difficult query conditions.

## Hypothetical Document Embeddings (HyDE)

*Purpose:* Use generated hypothetical documents to enhance retrieval for underspecified queries.

---

# RAG Evaluation & Quality Assessment

*Purpose:* Measure retrieval and generation quality using automated and LLM-based evaluation.

## RAG Evaluation with RAGAS

*Purpose:* Evaluate RAG pipelines using faithfulness, relevance, and context metrics.

## LLM-as-a-Judge Evaluation

*Purpose:* Assess answer quality using LLM-based judging frameworks.

## Deep Dive into RAGAS Metrics

*Purpose:* Analyze individual RAGAS metrics to diagnose system weaknesses.

---

# Deployed Prototype Solution -->

---

