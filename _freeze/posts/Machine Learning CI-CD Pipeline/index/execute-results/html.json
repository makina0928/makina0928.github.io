{
  "hash": "cdc116d09c435b810cd2f61d7c6240e9",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Modern Loan Portfolio Data Stack for Financial Institutions\"\nsubtitle: \"Improving portfolio health and minimizing loan defaults through intelligent automated data pipelines and real-time insights.\"\nauthor: \"Mulimbika Makina\"\ndate: 2025-11-13\nformat:\n  html:\n    toc: true\n    toc-depth: 2\n    number-sections: false \n    df-print: paged\ncategories: [Python, PostgreSQL, Dagster, dbt, Metabase, Docker]\n---\n\n![](ELT_Cover.jpg)\n\n<div style=\"background-color:#f5f7ff; padding:20px; border-radius:10px;\">\n# Overview\nIn the rapidly evolving financial sector, managing loan portfolios effectively is crucial for minimizing defaults and maximizing returns. This project presents a comprehensive solution that leverages modern data stack technologies to enhance loan portfolio management for financial institutions. We created an automated pipeline that provides real-time insights into portfolio health by integrating tools such as duckdb for data extraction, dlt for data loading, PostgreSQL for data storage, dbt for data transformation, Dagster for orchestration, and Metabase for visualization. This solution not only improves risk assessment and monitoring but also enables data-driven decision-making, ultimately leading to better financial outcomes.\n</div>\n\n::: {.callout-note}\nThis dashboard is purely for demonstration purposes. All data presented here were **synthetically generated** using Python’s `Faker` library to simulate realistic loan records. These figures **do not represent real customers or production data**.  \n\nFor access to a live dashboard please [Contact](https://zamdatalabs.com/contact.html).\n:::\n\n\n# Problem definition\nFinancial institutions often struggle with managing loan portfolios effectively, leading to increased default rates and financial losses. The challenge lies in accurately assessing borrower risk, monitoring portfolio health, and making informed lending decisions in real-time.\n\nTo address these challenges, we designed a robust ELT (Extract, Load, Transform) architecture that automates data processing and provides actionable insights. The architecture consists of the following key components:\n\n- **Data Extraction and Loading:** Utilized `duckdb` to efficiently query and merge multiple CSV files from local folders, followed by loading the consolidated data into a PostgreSQL database using the `dlt` library.\n\n- **Data Transformation:** Implemented `dbt` to clean, structure, and prepare the data for analysis, following the medallion architecture (bronze, silver, gold layers).\n\n- **Orchestration:** Employed `Dagster` to automate and schedule the entire data pipeline, ensuring seamless integration between extraction, transformation, and loading processes.\n\n- **Visualization:** Leveraged `Metabase` to create interactive dashboards that provide real-time insights into loan portfolio health, enabling stakeholders to make informed decisions.\n\n- **Containerization:** Used `Docker` to containerize the entire data stack, ensuring portability and ease of deployment across different environments.\n\nThe following diagram illustrates the overall ELT architecture employed in this project:\n\n# ELT architecture overview\n\n![](ELT_ARCHITECTURE.png)\n\n<!-- =================================================================================== -->\n# Data Extraction and Loading\nThe first step in our pipeline involved extracting data from local spreadsheet folders (3 source folders were used), including customers folder, location folder, and loan details folder. We utilized an open-source, in-process Online Analytical Processing (OLAP) database management system  (**duckdb**) to query multiple csv files from each folder by merging them using attribute names through efficient analytical duckdb queries. This approach streamlined data extraction (the \"E\" (Extract) in ELT) , ensuring consistency and reliability across the merged datasets.\n\nAfter consolidating multiple CSV files, the unified dataset (per folder) was ingested into a **PostgreSQL database** using the **data load tool (dlt)**. This open-source Python library simplifies data pipelines by automating the \"L\" (Load) in ELT workflows, making it fast and reliable to handle raw data.\n\n::: {.callout-tip title=\"Data Extraction & Loading Implementation Snippet\" collapse=true}\n\n::: {#3cdffd2b .cell execution_count=1}\n``` {.python .cell-code}\n#========================================#\n#   Importing required libraries         #\n#========================================#\n\nimport dlt\nimport duckdb\nimport os\nimport logging\nfrom pathlib import Path\nimport dotenv\n\n# ======================================== #\n#           Logging Setup                  #\n# ======================================== #\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | [%(levelname)s] | %(message)s\",\n)\nlog = logging.getLogger(__name__)\n\n# ======================================== #\n#   Environment Setup (.env for Postgres)  #\n# ======================================== #\ndotenv.load_dotenv()\nconnection_string = os.getenv(\"POSTGRES_CONNECTION_STRING\")\n\nif not connection_string:\n    raise ValueError(\" Missing POSTGRES_CONNECTION_STRING in your .env file!\")\n\npg_destination = dlt.destinations.postgres(connection_string)\n\n# ======================================== #\n#        Directory Setup                   #\n# ======================================== #\nbase_dir = Path(__file__).resolve().parent\ndata_dir = base_dir / \"Data\"\nfact_folder = data_dir / \"fact_db\"\ncustomer_folder = data_dir / \"customer\"\nlocation_folder = data_dir / \"location\"\n\n# Ensure staging truncation\ndlt.config[\"load.truncate_staging_dataset\"] = True\n\n\n# ======================================== #\n#            DLT Resources                 #\n# ======================================== #\n@dlt.resource(table_name=\"loans\", write_disposition=\"replace\")\ndef loans():\n    query = f\"\"\"\n        SELECT *\n        FROM read_csv_auto('{os.path.join(fact_folder, \"*.csv\")}', \n                           union_by_name = true, \n                           filename = true)\n    \"\"\"\n    con = duckdb.connect()\n    con.execute(query)\n    chunk_size = 10000\n    while True:\n        chunk = con.fetch_df_chunk(chunk_size)\n        if chunk is None or chunk.empty:\n            break\n        for record in chunk.to_dict(orient=\"records\"):\n            yield record\n\n\n@dlt.resource(table_name=\"customers\", write_disposition=\"replace\")\ndef customers():\n    query = f\"\"\"\n        SELECT *\n        FROM read_csv_auto('{os.path.join(customer_folder, \"*.csv\")}',\n                           union_by_name = true,\n                           filename = true)\n    \"\"\"\n    con = duckdb.connect()\n    con.execute(query)\n    chunk_size = 10000\n    while True:\n        chunk = con.fetch_df_chunk(chunk_size)\n        if chunk is None or chunk.empty:\n            break\n        for record in chunk.to_dict(orient=\"records\"):\n            yield record\n\n\n@dlt.resource(table_name=\"location\", write_disposition=\"replace\")\ndef locations():\n    query = f\"\"\"\n        SELECT *\n        FROM read_csv_auto('{os.path.join(location_folder, \"*.csv\")}',\n                           union_by_name = true,\n                           filename = true)\n    \"\"\"\n    con = duckdb.connect()\n    con.execute(query)\n    chunk_size = 10000\n    while True:\n        chunk = con.fetch_df_chunk(chunk_size)\n        if chunk is None or chunk.empty:\n            break\n        for record in chunk.to_dict(orient=\"records\"):\n            yield record\n\n\n@dlt.source\ndef source():\n    return [loans(), customers(), locations()]\n```\n:::\n\n\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Expand to explore other file formats DuckDB can directly query using SQL functions\n\n- File Formats: `Parquet`,` CSV`, `JSON`, `Excel (.xlsx files)`,` Text files`, `Blob files`, and `ZIP archives`.\n\n- In-Memory Data: `Pandas DataFrames`, `NumPy arrays`,`Polars DataFrames`, and `Apache Arrow tables`.\n\n- Databases: `PostgreSQL`,`MySQL`,  `SQLite`, and `Other DuckDB`.\n\n- Cloud and Remote Storage: `Amazon S3`, `Google Cloud Storage (GCS)`, `Azure Blob Storage`, and `HDFS`.\n\n- Web Data: `Web APIs` and `HTTP/HTTPS URLs`.\n\n- Geospatial Data: `GeoJSON` and `Shapefiles`.\n\n- Data Lake Formats: `Delta Lake` and `Iceberg`.\n\n***Ref:*** [DuckDB Documentation](https://duckdb.org/docs/stable/data/data_sources)\n:::\n\n\n<!-- ================================================================================== -->\n# Data Transformation with dbt\n\nAfter loading, the data was transformed using **dbt**, an open-source SQL-based tool that lets analysts and engineers clean, structure, and prepare data directly in their warehouse. In ELT workflows, dbt handles the \"T\" (Transform), turning raw data into analysis-ready datasets.\n\nUsing dbt allowed modular, reusable transformation logic (See @tip-jinja). The pipeline followed the **medallion architecture** where each layer was generated from the previous one, ensuring reliability, reusability, and analytical readiness. The layers included:\n\n* **Bronze (staging):** Cleans raw data.\n* **Silver (intermediate):** Standardizes and enriches data.\n* **Gold (marts):** Produces aggregated, business-ready insights.\n\n::: {.callout-tip title=\"dbt project configuration snippet\" collapse=true}\n```{.yaml code-fold=true}\n#=========================================#\n#        dbt_project.yml                  #\n#=========================================#\n\nname: 'postgres_dbt'\nversion: '1.0.0'\nprofile: 'postgres_dbt'\n# Model configurations\nmodels:\n  postgres_dbt:\n    src: \n      +schema: transformed\n      +materialized: ephemeral\n\n    dim:\n      +schema: transformed\n      +materialized: table\n\n    fct:\n      +schema: transformed\n      +materialized: table\n\n    marts:\n      +schema: analytics\n      +materialized: table\n```\n:::\n\n### 1. Source Definition & Staging Layer\n> *Purpose:* Acts as the **bronze layer**, ensuring that raw ingested data tables (declared in `sources.yml` file) is properly registered, validated, and queryable in dbt.\n\nThis layer defines the source tables ingested into PostgreSQL via dlt, enabling dbt to reference and transform them in subsequent models. The source tables include `customer`, `loans`, and `location`. In this project, we combined source and staging layer to ingest and perform preliminary raw data cleaning from the source tables (`customer`, `loans`, `location`), with minimal cleaning and schema alignment via SQL files.\n\n::: {.callout-note}\nIn a production scenario, the staging layer would typically involve more extensive data validation, cleansing, and transformation to ensure data quality before progressing to the silver layer. Therefore it is recommended to separate source definitions and staging transformations into distinct layers for better maintainability and clarity.\n:::\n\n::: {.callout-tip title=\"Source Definition Snippet\" collapse=true}\n```{.yaml code-fold=true}\n\n#========================================#\n#        sources.yml                     #\n#========================================#\n\nsources:\n  - name: portfolio_data\n    schema: staging\n    description: \"Raw data ingested from DLT pipeline containing customers, loans, and location tables.\"\n    tables:\n      - name: customers\n        description: \"Raw customer-level information.\"\n        meta:\n          dagster:\n            asset_key: [\"dlt_source_customers\"]\n\n      - name: loans\n        description: \"Raw loan-level portfolio data.\"\n        meta:\n          dagster:\n            asset_key: [\"dlt_source_loans\"]\n\n      - name: location\n        description: \"Raw location and relationship manager data.\"\n        meta:\n          dagster:\n            asset_key: [\"dlt_source_locations\"]\n```\n:::\n\n::: {.callout-tip title=\"Source Table Reference & Staging Layer Implementation Snippet\" collapse=true}\n```sql\n\n--========================================\n-- customer.sql      \n--========================================\nwith customers as (\n    select * \n    from {{ source('portfolio_data', 'customers') }}\n)\n\nselect\n    -- Convert customer ID to lowercase to ensure consistent format\n    TRIM(UPPER(customer_id)) as customer_id,\n    \n    -- Clean and standardize customer name\n    initcap(trim(customer_name)) as customer_name,\n    \n    -- Normalize gender\n    TRIM(UPPER(gender)) as gender,\n\n    -- standardize date of birth\n    CASE\n        WHEN dob::text ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n            THEN dob::date\n        ELSE NULL\n    END AS dob,\n\n    -- Clean and standardize categorical fields\n    trim(initcap(marital_status)) as marital_status,\n    trim(initcap(employment_status)) as employment_status,\n    \n    -- Normalize income bracket\n    trim(income_bracket) as income_bracket,\n    \n    -- Format region names uniformly\n    trim(initcap(region)) as region,\n    \n    -- Cast credit score to integer\n    cast(credit_score as int) as credit_score\n    \nfrom customers\n\n--========================================\n-- location.sql      \n--========================================\n\nwith locations as (\n    select * \n    from {{ source('portfolio_data', 'location') }}\n)\nSELECT\n    TRIM(UPPER(branch_id)) AS branch_id,\n    trim(initcap(branch_name)) AS branch_name,\n    trim(initcap(region)) AS region,\n    trim(initcap(country)) AS country,\n    trim(initcap(manager_name)) AS manager_name,\n    \n    CASE\n        WHEN opened_date::text ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n            THEN opened_date::date\n        ELSE NULL\n    END AS opened_date,\n\n    trim(initcap(branch_type)) AS branch_type\nFROM locations\n\n--========================================\n-- loans.sql      \n--========================================\n\nwith loans as (select * from {{ source('portfolio_data', 'loans') }})\n\nSELECT\n    TRIM(UPPER(loan_id)) AS loan_id,\n    TRIM(UPPER(customer_id)) AS customer_id,\n    TRIM(UPPER(branch_id)) AS branch_id,\n    trim(initcap(loan_product)) AS loan_product,\n\n    CASE\n        WHEN disbursement_date::text ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n            THEN disbursement_date::date\n        ELSE NULL\n    END AS disbursement_date,\n\n    CASE\n        WHEN maturity_date::text ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n            THEN maturity_date::date\n        ELSE NULL\n    END AS maturity_date,\n\n    CAST(loan_amount AS NUMERIC(12,2)) AS loan_amount,\n    CAST(interest_rate AS NUMERIC(5,2)) AS interest_rate,\n    CAST(installment_amount AS NUMERIC(12,2)) AS installment_amount,\n    CAST(outstanding_balance AS NUMERIC(12,2)) AS outstanding_balance,\n    CAST(loan_status AS VARCHAR) AS loan_status,\n    CAST(days_past_due AS NUMERIC(6,2)) AS days_past_due,\n    CAST(amount_overdue AS NUMERIC(12,2)) AS amount_overdue,\n\n    CASE\n        WHEN delinquency_start_date::text ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n            THEN delinquency_start_date::date\n        ELSE NULL\n    END AS delinquency_start_date,\n\n    CAST(default_flag AS INT) AS default_flag\nFROM loans\n\n```\n:::\n\n### 2. Transformation Layer (Dimensional & Fact Models)\n> *Purpose:* Serves as the **silver layer**, transforming raw data into structured and analysis-ready tables.\n\nThis layer transforms the cleaned staging data into structured, analysis-ready datasets. It includes dimension models (customer and location tables) and a fact model (loan table). The dimension models in this project standardized and enriched customer and location data, while the fact model consolidated loan details with calculated fields for delinquency analysis.\n\n::: {.callout-tip title=\"Transformation Layer Implementation Snippet\" collapse=true}\n```sql\n\n--========================================\n-- Transformation Model: customer.sql        \n--========================================\n\nwith customers as (\n    select *\n    from {{ ref('src_customer') }}\n    where customer_id is not null  -- Exclude invalid records\n      and trim(cast(customer_id as text)) <> ''   -- Exclude empty strings if any\n)\nselect\n    -- create a consistent surrogate key\n    {{ dbt_utils.generate_surrogate_key(['customer_id']) }} as sur_customer_id,\n\n    -- clean and standardized fields\n    customer_id,\n    initcap(trim(customer_name)) as customer_name,\n    case \n        when gender = 'MALE' then 'Male'\n        when gender = 'FEMALE' then 'Female'\n        else 'Unknown'\n    end as gender,\n    dob,\n    marital_status,\n    employment_status,\n    income_bracket,\n    region,\n    credit_score\nfrom customers\n\n--========================================\n-- Transformation Model: location.sql        \n--========================================\n\nwith locations as (\n    select *\n    from {{ ref('src_location') }}\n    where branch_id is not null  -- Exclude invalid records\n      and trim(cast(branch_id as text)) <> ''   -- Exclude empty strings if any\n)\n\nselect\n    -- create a consistent surrogate key\n    {{ dbt_utils.generate_surrogate_key(['branch_id']) }} as branch_id_sur,\n\n    -- original fields\n    branch_id,\n    branch_name,\n    region,\n    country,\n    manager_name,\n    opened_date,\n    branch_type\nfrom locations\n\n--========================================\n-- Transformation Model: loans.sql        \n--========================================\n\nwith loans as (\n    select *\n    from {{ ref('src_loans') }}\n      where loan_id is not null  -- Exclude invalid records\n      and trim(cast(loan_id as text)) <> ''   -- Exclude empty strings if any\n)\nselect\n    -- create consistent surrogate keys\n    {{ dbt_utils.generate_surrogate_key(['loan_id']) }} as sur_loan_id,\n    {{ dbt_utils.generate_surrogate_key(['customer_id']) }} as sur_customer_id,\n    {{ dbt_utils.generate_surrogate_key(['branch_id']) }} as sur_branch_id,\n\n    -- attributes\n    loan_id,\n    customer_id,\n    branch_id,\n    loan_product,\n    disbursement_date,\n    maturity_date,\n    loan_amount,\n    interest_rate,\n    installment_amount,\n    outstanding_balance,\n    loan_status,\n    days_past_due as days_late,\n    CASE WHEN days_past_due < 1 THEN days_past_due ELSE NULL END AS PAR_0,\n    CASE WHEN days_past_due >= 1 AND days_past_due < 30 THEN days_past_due ELSE NULL END AS PAR_30,\n    CASE WHEN days_past_due >= 30 AND days_past_due < 60 THEN days_past_due ELSE NULL END AS PAR_60,\n    CASE WHEN days_past_due >= 60 THEN days_past_due ELSE NULL END AS PAR_90,\n    amount_overdue,\n    delinquency_start_date,\n    default_flag\nfrom loans\n\n```\n:::\n\n\n### 3. Marts Layer \n> *Purpose:* Acts as the **gold layer**, delivering curated datasets for analytics and reporting.\n\nThis project utilized this layer to create business-facing datasets for analytics and machine learning data models. It consolidated dimension and fact models into an analytics table that supported Metabase dashboards and generated feature tables for ML models predicting loan defaults (future work).\n\n::: {.callout-tip title=\"Marts Layer Implementation Snippet\" collapse=true}\n```sql\n\n--========================================\n-- Analytics Model: Analytics.sql        \n--========================================\nWITH \n    -- Dimension tables\n    dim_customers AS (\n        SELECT\n            customer_id,\n            customer_name,\n            gender,\n            dob,\n            marital_status,\n            employment_status,\n            income_bracket\n        FROM {{ ref('customers') }}\n    ),\n\n    dim_location AS (\n        SELECT\n            branch_id,\n            branch_name,\n            region,\n            country,\n            manager_name,\n            opened_date,\n            branch_type\n        FROM {{ ref('location') }}\n    ),\n\n    -- Fact table (transactional data)\n    fact_loans AS (\n        SELECT\n            loan_id, \n            customer_id, \n            branch_id,             \n            loan_product,\n            disbursement_date,\n            maturity_date,\n            loan_amount AS disbursed_amt,\n            outstanding_balance AS outstanding_bal,\n            loan_status,\n            interest_rate,\n            days_late,\n            par_0,\n            par_30,\n            par_60,\n            par_90,\n            amount_overdue,\n            delinquency_start_date\n        FROM {{ ref('fact_tables') }}\n    )\n\n-- === STAR JOIN ===\nSELECT \n    -- Key alignment \n    f.customer_id AS fact_customer_id,\n    c.customer_id AS dim_customer_id,\n    f.branch_id AS fact_branch_id,\n    o.branch_id AS dim_branch_id,\n    f.loan_id,\n\n    -- Customer attributes\n    c.customer_name,\n    c.gender,\n    c.marital_status,\n    c.employment_status,\n    c.income_bracket,\n\n    -- Location attributes\n    o.branch_name,\n    o.region,\n    o.country,\n    o.manager_name,\n    o.branch_type,\n\n    -- Loan attributes\n    f.loan_product,\n    f.disbursement_date,\n    f.maturity_date,\n    f.disbursed_amt,\n    f.outstanding_bal,\n    f.interest_rate,\n    f.loan_status,\n    f.days_late,\n    f.par_0,\n    f.par_30,\n    f.par_60,\n    f.par_90,\n    f.amount_overdue,\n    f.delinquency_start_date,\n\n    -- Loan status flag\n    CASE \n        WHEN f.days_late >= 60 THEN 1\n        ELSE 0\n    END AS default_status,\n\n    -- Portfolio at Risk (PAR) ratio\n    ROUND(\n        COALESCE(f.par_30 + f.par_60 + f.par_90, 0)::NUMERIC \n        / NULLIF(f.outstanding_bal, 0), \n        4\n    ) AS portfolio_at_risk_ratio,\n\n    -- Derived time dimensions\n    EXTRACT(YEAR FROM f.disbursement_date) AS disbursement_year,\n    EXTRACT(MONTH FROM f.disbursement_date) AS disbursement_month,\n\n    -- Ranking by disbursement period\n    DENSE_RANK() OVER (\n        ORDER BY \n            EXTRACT(YEAR FROM f.disbursement_date),\n            EXTRACT(MONTH FROM f.disbursement_date)\n    ) AS disbursement_month_rank\n\nFROM fact_loans f\nINNER JOIN dim_customers c \n    ON f.customer_id = c.customer_id\nINNER JOIN dim_location o \n    ON f.branch_id = o.branch_id\n\n--========================================\n-- ML Model: training.sql        \n--========================================\n\n{% set train_start = '2022-01-01' %}\n{% set train_end = '2024-12-31' %}\n{% set observation_window_months = 6 %}\n\nWITH training AS (\n    SELECT\n        --ID columns \n        f.fact_customer_id,\n        f.dim_customer_id,\n        f.fact_branch_id,\n        f.dim_branch_id,\n\n        -- Loan details\n        f.loan_id,\n        f.loan_product,\n        f.disbursement_date,\n        f.disbursed_amt,\n        f.outstanding_bal,\n        f.interest_rate,\n\n        -- Customer attributes\n        f.gender,\n        f.marital_status,\n        f.employment_status,\n        f.income_bracket,\n\n        -- Branch attributes\n        f.region,\n        f.branch_type,\n\n        -- Loan performance metrics\n        f.days_late,\n        f.par_0,\n        f.par_30,\n        f.par_60,\n        f.par_90,\n\n        -- Target variable (label)\n        f.default_status AS target_default,\n\n        -- Derived date fields\n        f.disbursement_month,\n        f.disbursement_year\n\n    FROM {{ ref('analytics') }} AS f\n    WHERE\n        f.disbursement_date BETWEEN '{{ train_start }}' AND '{{ train_end }}'\n        -- Ensure at least full observation period before today\n        -- We only want to include loans that have had enough time to observe default behavior\n        AND f.disbursement_date <= CURRENT_DATE - INTERVAL '{{ observation_window_months }} months'\n)\n\nSELECT *\nFROM training\n\n--========================================\n-- ML Model: testing.sql        \n--========================================\n-- Testing/Evaluation set: loans disbursed from 2025-01-01 to 2025-10-01\n-- Must have full 6-month observation window\n\n{% set test_start = '2025-01-01' %}\n{% set test_end = '2025-10-01' %}\n{% set observation_window_months = 6 %}\n\nWITH testing AS (\n    SELECT\n        -- ID columns \n        f.fact_customer_id,\n        f.dim_customer_id,\n        f.fact_branch_id,\n        f.dim_branch_id,\n\n        -- Core loan attributes\n        f.loan_id,\n        f.loan_product,\n        f.disbursement_date,\n        f.disbursed_amt,\n        f.outstanding_bal,\n        f.interest_rate,\n\n        -- Customer attributes\n        f.gender,\n        f.marital_status,\n        f.employment_status,\n        f.income_bracket,\n\n        -- Branch attributes\n        f.region,\n        f.branch_type,\n\n        -- Performance metrics and target label\n        f.days_late,\n        f.par_0,\n        f.par_30,\n        f.par_60,\n        f.par_90,\n        f.default_status AS target_default,\n\n        --  Derived date fields\n        f.disbursement_month,\n        f.disbursement_year\n\n    FROM {{ ref('analytics') }} AS f\n    WHERE \n        f.disbursement_date BETWEEN '{{ test_start }}' AND '{{ test_end }}'\n        -- Ensure full observation period (at least 6 months since disbursement)\n        AND f.disbursement_date <= CURRENT_DATE - INTERVAL '{{ observation_window_months }} months'\n)\n\nSELECT *\nFROM testing\n\n\n--========================================\n-- ML Model: inference.sql        \n--========================================\n-- Inference set: NEW loans disbursed in November 2025\n-- These loans have no target label (yet)\n-- Only include features known at disbursement\n\n{% set inference_month_start = '2025-11-01' %}\n{% set inference_month_end = '2025-11-30' %}\n\nWITH inference AS (\n    SELECT\n        -- ID columns\n        f.fact_customer_id,\n        f.dim_customer_id,\n        f.fact_branch_id,\n        f.dim_branch_id,\n\n        -- Loan details\n        f.loan_id,\n        f.loan_product,\n        f.disbursement_date,\n        f.disbursed_amt,\n        f.outstanding_bal,\n        f.interest_rate,\n\n        --  Customer attributes\n        f.gender,\n        f.marital_status,\n        f.employment_status,\n        f.income_bracket,\n\n        -- Location attributes\n        f.region,\n        f.branch_type,\n\n        -- Derived date columns\n        f.disbursement_month,\n        f.disbursement_year\n\n    FROM {{ ref('analytics') }} AS f\n    WHERE\n        f.disbursement_date BETWEEN '{{ inference_month_start }}' AND '{{ inference_month_end }}'\n        -- Exclude any loans already defaulted (we only want active/new)\n        AND f.default_status = 0\n)\n\nSELECT *\nFROM inference\n\n```\n:::\n\n\n::: {#tip-jinja .callout-tip}\n## SQL Templating with Jinja\n**Jinja templating** throughout the models ensured dynamic SQL, avoiding code duplication, and guaranteed **scalability and consistency** across the pipeline.\n:::\n\n<!-- ============================================================================== -->\n# Orchestration with Dagster\nTo automate the entire data pipeline, we employed Dagster, a powerful orchestration tool. Dagster enabled the definition and scheduling of workflows that encompass data extraction, transformation, and loading processes. By setting up Dagster jobs ensured that the pipeline runs seamlessly and consistently, providing up-to-date insights into the loan portfolio. The figure below illustrates the Dagster pipeline architecture used in this project.\n\n**Dagster Pipeline Visualization:**\n\n![](elt_engineering.svg)\n\n::: {.callout-tip title=\"Pipeline Orchestration with Dagster Snippet\" collapse=true}\n\n::: {#7c83dcc6 .cell execution_count=2}\n``` {.python .cell-code}\n# ==================== #\n#       Imports        #\n# ==================== #\n\nfrom pathlib import Path\nimport sys\nimport os\nimport logging\nfrom dotenv import load_dotenv\nimport dlt\nimport dagster as dg\nfrom dagster_dlt import DagsterDltResource, dlt_assets\nfrom dagster_dbt import DbtCliResource, dbt_assets, DbtProject\n\n\n# ==================== #\n#    Logging Setup     #\n# ==================== #\nBASE_DIR = Path(__file__).parents[1]\nLOGS_DIR = BASE_DIR / \"logs\"\nLOGS_DIR.mkdir(exist_ok=True)\n\nlog_file = LOGS_DIR / \"dagster_orchestration.log\"\n\nlogging.basicConfig(\n    filename=log_file,\n    level=logging.INFO,\n    format=\"%(asctime)s | [%(levelname)s] | %(message)s\",\n)\n\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\nconsole.setFormatter(logging.Formatter(\"%(asctime)s | [%(levelname)s] | %(message)s\"))\nlogging.getLogger().addHandler(console)\n\nlogging.info(\" Starting Dagster orchestration setup...\")\n\n\n# ==================== #\n#        Setup         #\n# ==================== #\n\n# Add ETL base directory to sys.path for imports\nsys.path.insert(0, str(BASE_DIR))\nfrom data_extraction.load_data import source  # Import the source function\nlogging.info(\" Imported DLT source from data_extraction.load_data\")\n\n# Load environment variables from .env file\nENV_PATH = BASE_DIR / \".env\"\nload_dotenv(dotenv_path=ENV_PATH)\n\nconnection_string = os.getenv(\"POSTGRES_CONNECTION_STRING\")\nif not connection_string:\n    logging.error(\" Missing POSTGRES_CONNECTION_STRING in .env file\")\n    raise ValueError(\"Missing POSTGRES_CONNECTION_STRING in .env\")\n\nlogging.info(f\" Using PostgreSQL connection: {connection_string}\")\n\n# Initialize Dagster DLT resource\ndlt_resource = DagsterDltResource()\n\n\n# ==================== #\n#       DLT ASSET      #\n# ==================== #\n@dlt_assets(\n    dlt_source=source(),  \n    dlt_pipeline=dlt.pipeline(\n        pipeline_name=\"loans_pipeline\",\n        dataset_name=\"staging\",\n        destination=dlt.destinations.postgres(connection_string)\n    ),\n)\ndef dlt_load(context: dg.AssetExecutionContext, dlt: DagsterDltResource):\n    \"\"\"Load local CSVs (customers, loans, location) into PostgreSQL via DLT pipeline.\"\"\"\n    logging.info(\" [DLT] Starting pipeline execution through Dagster...\")\n    try:\n        yield from dlt.run(context)  # Execute the DLT pipeline\n        logging.info(\" [DLT] Pipeline completed successfully via Dagster.\")\n    except Exception as e:\n        logging.exception(f\" [DLT] Pipeline execution failed: {e}\")\n        raise\n\n\n# ==================== #\n#       DBT ASSET      #\n# ==================== #\ndbt_project_dir = BASE_DIR / \"postgres_dbt\"\nprofiles_dir = dbt_project_dir  # profiles.yml is here\n\ndbt_project = DbtProject(project_dir=dbt_project_dir, profiles_dir=profiles_dir)\ndbt_resource = DbtCliResource(project_dir=dbt_project_dir, profiles_dir=profiles_dir)\n\ndbt_project.prepare_if_dev()\nlogging.info(\" [DBT] Project and profiles initialized successfully\")\n\n@dbt_assets(manifest=dbt_project.manifest_path)\ndef dbt_models(context: dg.AssetExecutionContext, dbt: DbtCliResource):\n    \"\"\"Run and expose all dbt models (staging → transformed → marts).\"\"\"\n    logging.info(\" [DBT] Running dbt build...\")\n    try:\n        yield from dbt.cli([\"build\"], context=context).stream()\n        logging.info(\" [DBT] dbt build completed successfully.\")\n    except Exception as e:\n        logging.exception(f\" [DBT] dbt build failed: {e}\")\n        raise\n\n\n# ==================== #\n#         JOBS         #\n# ==================== #\n\n# Define a job for DLT loading\njob_dlt = dg.define_asset_job(\n    \"job_dlt\",\n    selection=dg.AssetSelection.keys(\n        \"dlt_source_loans\",      # These are the actual asset keys created by @dlt_assets\n        \"dlt_source_customers\", \n        \"dlt_source_locations\"\n    ),\n)\n\n# Define a job for dbt transformations\njob_dbt = dg.define_asset_job(\n    \"job_dbt\",\n    selection=dg.AssetSelection.key_prefixes(\"staging\", \"transformed\", \"marts\")\n)\n\n\n# ==================== #\n#       SCHEDULES      #\n# ==================== #\nschedule_dlt = dg.ScheduleDefinition(\n    job=job_dlt,\n    cron_schedule=\"10 1 * * *\"  # 01:10 UTC = 04:10 Nairobi\n)\nlogging.info(\"Dagster schedule created for job_dlt (daily 04:10 Nairobi)\")\n\n\n# ==================== #\n#       SENSORS        #\n# ==================== #\n@dg.multi_asset_sensor(\n    monitored_assets=[\n        dg.AssetKey(\"dlt_source_customers\"),\n        dg.AssetKey(\"dlt_source_loans\"),\n        dg.AssetKey(\"dlt_source_locations\"),\n    ],\n    job=job_dbt,\n)\ndef dlt_load_sensor(context):\n    \"\"\"Trigger dbt job after all DLT ingestion assets complete.\"\"\"\n    logging.info(\"⚡ Sensor triggered: DLT assets finished loading. Launching dbt job...\")\n    yield dg.RunRequest(run_key=None)\n\n\n# ==================== #\n#      DEFINITIONS     #\n# ==================== #\ndefs = dg.Definitions(\n    assets=[dlt_load, dbt_models],\n    resources={\"dlt\": dlt_resource, \"dbt\": dbt_resource},\n    jobs=[job_dlt, job_dbt],\n    schedules=[schedule_dlt],\n    sensors=[dlt_load_sensor]\n)\n\nlogging.info(\" Dagster definitions initialized successfully.\")\nlogging.info(\" Dagster orchestration setup complete — ready for `dagster dev`.\")\n```\n:::\n\n\n:::\n\n\n<!-- ============================================================================== -->\n# Dockerization\nTo ensure that our data stack is portable and easy to deploy, we containerized each component using Docker. This approach allowed us to package the PostgreSQL database, dbt transformations, Dagster orchestration, and Metabase visualization into containers. Docker Compose was employed to manage multi-container applications, allowing us to define and run the entire data stack with a single command. This setup not only simplified deployment but also enhanced scalability and maintainability.\n\n::: {.callout-tip title=\"Docker Compose Snippet\" collapse=true}\n```yaml\n#========================================#\n#        docker-compose.yml               #\n#========================================#\nservices:\n  dwh_pipeline:\n    build:\n      context: .\n      dockerfile: Dockerfile.dwh\n    container_name: dwh_pipeline\n    env_file:\n      - .env\n    environment:\n      CREDENTIALS__CONNECTION_STRING: ${POSTGRES_CONNECTION_STRING}\n      DBT_HOST: ${DBT_HOST}\n      DBT_USER: ${DBT_USER}\n      DBT_PASSWORD: ${DBT_PASSWORD}\n      DBT_PORT: ${DBT_PORT}\n      DBT_DBNAME: ${DBT_DBNAME}\n      DBT_SCHEMA: ${DBT_SCHEMA}\n    volumes:\n      # Existing data mount\n      - ./data_extraction/Data:/pipeline/data_extraction/Data\n\n      # Additional volumes for runtime editing and live data\n      - ./data_extraction:/pipeline/data_extraction\n      - ./postgres_dbt:/pipeline/postgres_dbt\n      - ./orchestration:/pipeline/orchestration\n\n      # Mount dbt profile directly \n      - ./postgres_dbt/profiles.yml:/root/.dbt/profiles.yml\n\n      # Mount global data directory \n      - ./data:/pipeline/data\n\n      # Mount requirements file for live dependency changes\n      - ./requirements.txt:/pipeline/orchestration/requirements.txt\n\n      # Mount .env file for consistent runtime environment\n      - ./.env:/pipeline/.env\n\n    expose:\n      - 3001\n    depends_on:\n      - postgres_etl\n    networks:\n      - dokploy-network\n    labels:\n      # Network labels are deliberately removed for security purposes\n\n  postgres_etl:\n    image: postgres:latest\n    container_name: postgres_etl\n    environment:\n      POSTGRES_USER: ${DBT_USER}\n      POSTGRES_DB: ${DBT_DBNAME}\n      POSTGRES_PASSWORD: ${DBT_PASSWORD}\n    volumes:\n      - etl_pgdata:/var/lib/postgresql/data\n    networks:\n      - dokploy-network\n\nvolumes:\n  etl_pgdata:\n\nnetworks:\n  dokploy-network:\n    external: true\n```\n:::\n\n::: {.callout-note}\n[Contact](https://zamdatalabs.com/contact.html) to access the deployed warehouse pipeline.\n:::\n\n<!-- ============================================================================== -->\n# Conclusion\nThis project demonstrates the effectiveness of a modern data stack in managing loan portfolios for financial institutions. By integrating PostgreSQL, dbt, Dagster, and Metabase, we created an automated pipeline that enhances data management, transformation, and visualization. The solution provides real-time insights into portfolio health, enabling better risk assessment and decision-making. Future work will focus on incorporating machine learning models to predict borrower defaults to further enhance portfolio management strategies.\n\n<!-- ================================================================================= -->\n# Future Work\nFuture enhancements to this project could include the integration of machine learning algorithms to predict loan defaults based on historical data. Additionally, expanding the data sources to include alternative credit data and social media insights could provide a more comprehensive view of borrower risk. Implementing advanced analytics and reporting features in Metabase would also enhance the user experience and provide deeper insights into portfolio performance.\n\n<!-- ===================================================================================== -->\n# Acknowledgments\nI would like to express my gratitude to the open-source community for providing the tools and resources that made this project possible. Special thanks to the developers of PostgreSQL, dbt, Dagster, and Metabase for their continuous efforts in advancing data management and analytics.\n\n<!-- ===================================================================================== -->\n# References\n\n- [dlt](https://dlthub.com/docs/intro)\n- [duckdb](https://duckdb.org/)\n- [dbt](https://docs.getdbt.com/docs/core/installation-overview)\n- [dagster](https://dagster.io/)\n- [metabase](https://www.metabase.com/)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}