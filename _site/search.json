[
  {
    "objectID": "posts/RAG/index.html#document-ingestion-parsing",
    "href": "posts/RAG/index.html#document-ingestion-parsing",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "3.1 Document Ingestion & Parsing",
    "text": "3.1 Document Ingestion & Parsing\nThe pipeline begins with raw documents parsing, where documents are converted into clean, structured text before any downstream processing is possible. This project uses llama-parse to convert a wide range of document formats into structured markdown, preserving headings and layout information.\n\n3.1.1 Core Features & Parsing Fundamentals\nLlamaParse is a GenAI-native document parser designed for converting complex documents into LLM-ready data for Retrieval-Augmented Generation (RAG) systems and it excels at:\n\nComplex Document Handling: Financial reports, research papers, scanned PDFs\nPrecise Extraction: Tables, charts, images, and diagrams\nLLM-Ready Output: Clean markdown, text, or structured JSON\n\n\n3.1.1.1 Supported File Formats (70+)\n\n\n\nCategory\nFormats\n\n\n\n\nDocuments\nPDF, DOC, DOCX, RTF, TXT, EPUB\n\n\nSpreadsheets\nXLSX, XLS, CSV, ODS\n\n\nPresentations\nPPTX, PPT\n\n\nImages\nJPG, PNG, GIF, BMP, TIFF, WEBP, SVG\n\n\nWeb\nHTML, HTM\n\n\nAudio\nMP3, MP4, WAV, WEBM, M4A (≤20MB)\n\n\n\n\n\n3.1.1.2 Pricing\n\nFree Tier: 1,000 pages daily\nPaid Tier: 7,000 pages/week + $0.003/additional page\n\n\n\n3.1.1.3 API Key Setup\nLlamaParse requires an API key from LlamaCloud.\nSteps to get your API key:\n\nGo to https://cloud.llamaindex.ai/\nSign up or log in\nNavigate to API Keys section\nCreate a new API key (starts with llx-)\n\n\n\n3.1.1.4 Architecture\n\nLlamaParse provides both synchronous and asynchronous methods:\n\n\n\n\nSync Method\nAsync Method\nDescription\n\n\n\n\nload_data()\naload_data()\nParse and return Documents\n\n\nparse()\naparse()\nParse and return JobResult\n\n\nget_images()\naget_images()\nGet extracted images\n\n\n\n\nLlamaParse uses a job-based architecture:\n\nDocument → Submit Job → Poll Status → Get Results\n\nKey Components:\nLlamaParse Client: Main interface for document parsing\nJob: Represents a parsing task (can be async)\nJobResult: Contains parsed content, pages, images, charts, layout\nDocument: LlamaIndex Document object with text and metadata\nAvailable pre-optimized configurations (presets) for different document types\n\n\n\n\nPreset\nBest For\nDescription\n\n\n\n\nfast\nQuick extraction\nNo OCR, fastest processing\n\n\nbalanced\nGeneral documents\nBalance of speed and accuracy\n\n\npremium\nComplex documents\nBest quality, uses advanced models\n\n\nstructured\nForms, tables\nOptimized for structured data\n\n\nauto\nMixed content\nAutomatic mode selection\n\n\nscientific\nResearch papers\nLaTeX, equations, citations\n\n\ninvoice\nInvoices, receipts\nFinancial document extraction\n\n\nslides\nPresentations\nPowerPoint, slide content\n\n\n\n\n\n3.1.1.5 Parse Modes\n\n1. High-Level Modes (Boolean flags):\nfast_mode: Skip OCR, fastest processing\npremium_mode: Best available parser\nauto_mode: Automatic mode selection\n2. Granular Parse Modes (Page-level):\nparse_page_without_llm: Fast extraction without AI\nparse_page_with_llm: Uses LLM for each page\nparse_page_with_lvm: Uses vision model for pages\nparse_page_with_agent: Agentic reasoning per page\nparse_page_with_layout_agent: Layout-aware agent"
  },
  {
    "objectID": "posts/RAG/index.html#chunking-the-document",
    "href": "posts/RAG/index.html#chunking-the-document",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "3.2 Chunking the Document",
    "text": "3.2 Chunking the Document\nLarge documents are split into smaller, semantically meaningful chunks. This is essential because:\n\nEmbedding models have input limits\nSmaller chunks improve retrieval accuracy\n\nThis project uses Chonkie’s RecursiveChunker, which is character-based, LLM-agnostic and production-safe. Chunks are created with:\n\nA fixed size (2048 characters)\nMinimum length enforcement\nStable character offsets for traceability\n\n\n3.2.1 Why this choice\nChonkie is a production-ready text chunking library designed specifically for RAG applications. It provides:\n\n9 specialized chunkers (RecursiveChunker, TokenChunker, SentenceChunker, TableChunker, CodeChunker, SemanticChunker, LateChunker, NeuralChunker, SlumberChunker)\nLocal processing - your data never leaves your infrastructure\nHigh performance - optimized for speed and efficiency\nThread-safe - suitable for concurrent processing\nFlexible embeddings - works with OpenAI, Gemini, Sentence Transformers, and more"
  },
  {
    "objectID": "posts/RAG/index.html#embedding-generation",
    "href": "posts/RAG/index.html#embedding-generation",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "3.3 Embedding Generation",
    "text": "3.3 Embedding Generation\nIn this project, each chunk is converted into a numerical vector (embedding) using Sentence Transformers (local) to capture each chunk’s semantic meaning (other alternatives include OpenAI, Gemini). The same process is later applied to user queries. 768-dimensional embeddings are generated locally and cached to avoid recomputation.\n\n3.3.1 Why this choice\n\nNo external API dependency\nPredictable cost (zero per-request fees)\nEasy to swap models\nSuitable for private environments"
  },
  {
    "objectID": "posts/RAG/index.html#vector-storage-retrieval",
    "href": "posts/RAG/index.html#vector-storage-retrieval",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "3.4 Vector Storage & Retrieval",
    "text": "3.4 Vector Storage & Retrieval\nAll chunk embeddings are stored in a vector database. At query time, the system retrieves the most relevant chunks using similarity search (Top-K document chunks are ranked by semantic similarity). This project uses Qdrant as the vector store, with the following key features:\n\nCosine similarity\nMetadata filtering (namespace, filename)\nPayload indexing\nAsync client for scalability\n\n\n3.4.1 Why this choice\n\nEasy to self-host (Open-source) and cloud hosted (with 1GB free cluster).\nStrong filtering and indexing support\nClean Python API"
  },
  {
    "objectID": "posts/RAG/index.html#context-construction",
    "href": "posts/RAG/index.html#context-construction",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "3.5 Context Construction",
    "text": "3.5 Context Construction\nRetrieved chunks are formatted into a single context block, preserving:\n\nSource filename\nRelevance score\nOptional heading hierarchy\n\nThis context becomes the grounding evidence for the LLM and is used to generate the final answer.\n\n3.5.1 Why this matters\n\nImproves answer quality\nEnables traceability\nReduces hallucination risk"
  },
  {
    "objectID": "posts/RAG/index.html#augmented-prompt-generation",
    "href": "posts/RAG/index.html#augmented-prompt-generation",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "3.6 Augmented Prompt Generation",
    "text": "3.6 Augmented Prompt Generation\nThe user’s question and the retrieved context are combined into a single prompt. The prompt explicitly instructs the LLM to use only the provided context and admit when information is missing. A grounded, constrained prompt is then sent to the LLM."
  },
  {
    "objectID": "posts/RAG/index.html#answer-generation",
    "href": "posts/RAG/index.html#answer-generation",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "3.7 Answer Generation",
    "text": "3.7 Answer Generation\nThe LLM generates the final answer using the augmented prompt. This implementation uses Groq LLM APIs for fast inference, but the architecture is model-agnostic. A clear, grounded answer with optional source citations is returned to the user."
  },
  {
    "objectID": "posts/RAG/index.html#artifact-cache-storage",
    "href": "posts/RAG/index.html#artifact-cache-storage",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "3.8 Artifact & Cache Storage",
    "text": "3.8 Artifact & Cache Storage\nTo improve performance and reproducibility, intermediate artifacts are stored in Cloudflare R2 which is a low-cost object storage service and is S3-compatible. The following artifacts are stored:\n\nOriginal documents\nChunks\nEmbeddings\nMetadata\n\n\n3.8.1 Why this matters\n\nFaster reloads\nStateless services\nEasy reindexing\nLower compute cost (with 10 GB free storage)"
  },
  {
    "objectID": "posts/RAG/index.html#summary",
    "href": "posts/RAG/index.html#summary",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "3.9 Summary",
    "text": "3.9 Summary\nThis RAG stream turns raw, private documents into grounded, auditable answers by combining:\n\nRobust document parsing\nDeterministic chunking\nLocal embedding generation\nVector-based retrieval\nContext-aware LLM prompting\n\nMost importantly, the LLM is never treated as a source of truth. It is a reasoning layer operating strictly on retrieved evidence."
  },
  {
    "objectID": "posts/RAG/index.html#user-question",
    "href": "posts/RAG/index.html#user-question",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "4.1 User Question",
    "text": "4.1 User Question\nA user submits a natural language question that requires database access. At this point, the system does not assume the question is valid or executable."
  },
  {
    "objectID": "posts/RAG/index.html#llm-as-a-reasoning-engine",
    "href": "posts/RAG/index.html#llm-as-a-reasoning-engine",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "4.2 LLM as a Reasoning Engine",
    "text": "4.2 LLM as a Reasoning Engine\nThe LLM receives the user question along with a strict system prompt that defines its role. In this project, the LLM is not a data source and not allowed to answer directly.\nIts responsibilities are limited to:\n\nUnderstanding the user’s intent\nPlanning how to explore the database\nDeciding which tools to call and in what order\nExplaining its reasoning for every tool call\n\nThis separation ensures that:\n\nThe LLM cannot hallucinate results\nAll answers must be grounded in actual database queries\nEvery step is explainable and auditable"
  },
  {
    "objectID": "posts/RAG/index.html#agent-control-loop",
    "href": "posts/RAG/index.html#agent-control-loop",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "4.3 Agent Control Loop",
    "text": "4.3 Agent Control Loop\nThe SQL Agent runs inside a controlled reasoning loop, where each iteration performs the following steps:\n\nThe LLM proposes tool calls\nTools are executed by the system (not the LLM)\nTool outputs are fed back to the LLM\nThe loop continues until a final answer is produced\n\nThis loop is bounded by a maximum number of iterations and hard failure conditions. This design prevents:\n\nInfinite reasoning loops\nUnbounded database exploration\nUncontrolled compute costs"
  },
  {
    "objectID": "posts/RAG/index.html#guardrails-constraints",
    "href": "posts/RAG/index.html#guardrails-constraints",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "4.4 Guardrails & Constraints",
    "text": "4.4 Guardrails & Constraints\nBefore any SQL is executed, the system enforces hard safety rules that the LLM cannot override.\nThese include:\n\nRead-only enforcement\n\nNo INSERT, UPDATE, DELETE, DROP, ALTER, or CREATE\n\nMandatory LIMIT clause\n\nPrevents large result sets\n\nSchema validation\n\nOnly known tables and columns are allowed\n\nTool usage enforcement\n\nThe LLM must call at least one database tool\n\nTemporal query guards\n\nPrevents ambiguous “current” or “latest” queries without validation\n\n\nIf any rule is violated, execution stops immediately."
  },
  {
    "objectID": "posts/RAG/index.html#database-tools",
    "href": "posts/RAG/index.html#database-tools",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "4.5 Database Tools",
    "text": "4.5 Database Tools\nThe LLM can request a limited set of explicit tools, each designed for a specific purpose:\n\nList Tables\n\nDiscover available tables\n\nDescribe Tables\n\nInspect schema and columns\n\nSample Data\n\nPreview small subsets of rows\n\nExecute Read-Only SQL\n\nRun validated, constrained queries\n\n\nEach tool:\n\nRequires a reasoning parameter\nRuns inside controlled Python code\nUses a secure PostgreSQL connection\n\n\n4.5.1 Why this matters\n\nTools act as the only gateway to the database.\nThe LLM never touches SQL execution directly."
  },
  {
    "objectID": "posts/RAG/index.html#protected-database-access",
    "href": "posts/RAG/index.html#protected-database-access",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "4.6 Protected Database Access",
    "text": "4.6 Protected Database Access\nAll database operations go through a protected PostgreSQL connection with:\n\nRead-only enforcement\nCursor-level control\nAutomatic rollback on failure\nLimited result sizes\n\nThe database is treated as a protected system, not an LLM playground."
  },
  {
    "objectID": "posts/RAG/index.html#interpreting-query-results",
    "href": "posts/RAG/index.html#interpreting-query-results",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "4.7 Interpreting Query Results",
    "text": "4.7 Interpreting Query Results\nTool results are returned to the LLM as structured messages then the LLM:\n\nInterpret the data\nSummarize insights\nTranslate technical results into business-friendly language\n\nCrucially, the LLM does not fabricate numbers and if no relevant data is found, it must say so explicitly."
  },
  {
    "objectID": "posts/RAG/index.html#final-answer-to-the-user",
    "href": "posts/RAG/index.html#final-answer-to-the-user",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "4.8 Final Answer to the User",
    "text": "4.8 Final Answer to the User\nOnce the LLM has:\n\nUsed at least one database tool\nPassed all guardrails\nInterpreted real query results\n\nIt produces a final answer that:\n\nIs grounded in actual data\nIs formatted in clear Markdown\nAvoids exposing raw SQL unless requested\nIs understandable to non-technical users\n\n\nThis SQL Agent stream complements the RAG stream by enabling safe, explainable access to structured data, while preserving all the reliability guarantees required for real-world systems. Together, the RAG stream and SQL Agent stream form a unified knowledge access layer where:\n\nDocuments are retrieved, not guessed\nDatabases are queried, not hallucinated\nLLMs reason, but never act unchecked"
  },
  {
    "objectID": "posts/RAG/index.html#user-asks-a-question",
    "href": "posts/RAG/index.html#user-asks-a-question",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "5.1 User Asks a Question",
    "text": "5.1 User Asks a Question\nThe flow begins when a user submits a natural-language question. At this point, the system does not assume where the answer should come from."
  },
  {
    "objectID": "posts/RAG/index.html#api-gateway-receives-the-request",
    "href": "posts/RAG/index.html#api-gateway-receives-the-request",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "5.2 API Gateway Receives the Request",
    "text": "5.2 API Gateway Receives the Request\nThe API Gateway serves as the single entry point for all queries. Its responsibilities include:\n\nAccept the user question\nAttach session and request metadata\nForward the question to the routing service\n\nNo reasoning or data access happens here."
  },
  {
    "objectID": "posts/RAG/index.html#routing-service-decides-the-best-path",
    "href": "posts/RAG/index.html#routing-service-decides-the-best-path",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "5.3 Routing Service Decides the Best Path",
    "text": "5.3 Routing Service Decides the Best Path\nThe Routing Service analyzes the question and determines which execution path is required:\n\nData Question → Requires structured database access → Route to the SQL Agent\nKnowledge Question → Requires document understanding → Route to the RAG pipeline\nMixed Question → Requires both documents and database facts → Route to the hybrid execution path"
  },
  {
    "objectID": "posts/RAG/index.html#execution-paths",
    "href": "posts/RAG/index.html#execution-paths",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "5.4 Execution Paths",
    "text": "5.4 Execution Paths\n\n5.4.1 SQL Agent Path (Structured Data)\nFor data-driven questions:\n\nThe SQL Agent uses the LLM as a reasoning engine\nThe LLM plans database exploration steps\nDatabase tools are executed under strict guardrails:\n\nRead-only queries\nSchema validation\nAutomatic limits\nMandatory tool usage\n\nThe database remains fully protected\n\nThe LLM never executes SQL directly, it only reasons about which tool should run next.\n\n\n\n5.4.2 RAG Path (Unstructured Knowledge)\nFor document-driven questions:\n\nRelevant documents are parsed and chunked\nEmbeddings are generated using a local embedding model\nSimilar chunks are retrieved from the vector database\nRetrieved context is combined with the user question\nThe LLM generates an answer grounded strictly in retrieved content\n\nThis ensures answers are evidence-based, not hallucinated.\n\n\n\n5.4.3 Hybrid Path (Knowledge + Database)\nFor mixed questions:\n\nThe system retrieves relevant document context and\nExecutes validated SQL queries via the SQL Agent\nBoth sources are combined into a single augmented context\nThe LLM generates a unified answer that:\n\nExplains what the documents say\nReferences what the data shows\n\n\nThis allows true cross-source reasoning without compromising safety."
  },
  {
    "objectID": "posts/RAG/index.html#data-knowledge-layer",
    "href": "posts/RAG/index.html#data-knowledge-layer",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "5.5 Data & Knowledge Layer",
    "text": "5.5 Data & Knowledge Layer\nAll execution paths ultimately draw from the same Data & Knowledge layer, which includes:\n\nBusiness Database (PostgreSQL)\nKnowledge Base (vector store)\nCombined Knowledge + Database context (for hybrid queries)\n\nThis layer is treated as the source of truth, never the LLM."
  },
  {
    "objectID": "posts/RAG/index.html#generate-a-clear-answer",
    "href": "posts/RAG/index.html#generate-a-clear-answer",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "5.6 Generate a Clear Answer",
    "text": "5.6 Generate a Clear Answer\nOnce relevant data is retrieved:\n\nThe LLM interprets results\nConverts technical outputs into human-readable explanations\nAvoids fabricating missing information\nExplicitly states when data is insufficient\n\nThe answer is formatted for clarity and trust, not verbosity."
  },
  {
    "objectID": "posts/RAG/index.html#return-answer-to-user",
    "href": "posts/RAG/index.html#return-answer-to-user",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "5.7 Return Answer to User",
    "text": "5.7 Return Answer to User\nThe final response is returned to the user with:\n\nClear reasoning\nGrounded evidence\nNo hidden assumptions\nNo hallucinated facts\n\nFrom the user’s perspective, this feels like a single intelligent assistant but internally, it is a carefully orchestrated system."
  },
  {
    "objectID": "posts/RAG/index.html#database-connection-view",
    "href": "posts/RAG/index.html#database-connection-view",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "6.1 Database Connection view",
    "text": "6.1 Database Connection view"
  },
  {
    "objectID": "posts/RAG/index.html#document-retrieval-view",
    "href": "posts/RAG/index.html#document-retrieval-view",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "6.2 Document Retrieval view",
    "text": "6.2 Document Retrieval view"
  },
  {
    "objectID": "posts/RAG/index.html#vector-search-view",
    "href": "posts/RAG/index.html#vector-search-view",
    "title": "Hybrid RAG-SQL Agent for Enterprise Knowledge Systems",
    "section": "6.3 Vector Search view",
    "text": "6.3 Vector Search view\n\nContact me to request access to the live demo."
  },
  {
    "objectID": "posts/Machine Learning CI-CD Pipeline/index.html",
    "href": "posts/Machine Learning CI-CD Pipeline/index.html",
    "title": "Modern Loan Portfolio Data Stack for Financial Institutions",
    "section": "",
    "text": "Overview\nIn the rapidly evolving financial sector, managing loan portfolios effectively is crucial for minimizing defaults and maximizing returns. This project presents a comprehensive solution that leverages modern data stack technologies to enhance loan portfolio management for financial institutions. We created an automated pipeline that provides real-time insights into portfolio health by integrating tools such as duckdb for data extraction, dlt for data loading, PostgreSQL for data storage, dbt for data transformation, Dagster for orchestration, and Metabase for visualization. This solution not only improves risk assessment and monitoring but also enables data-driven decision-making, ultimately leading to better financial outcomes.\n\n\n\n\n\n\n\nNote\n\n\n\nThis dashboard is purely for demonstration purposes. All data presented here were synthetically generated using Python’s Faker library to simulate realistic loan records. These figures do not represent real customers or production data.\nFor access to a live dashboard please Contact.\n\n\n\nProblem definition\nFinancial institutions often struggle with managing loan portfolios effectively, leading to increased default rates and financial losses. The challenge lies in accurately assessing borrower risk, monitoring portfolio health, and making informed lending decisions in real-time.\nTo address these challenges, we designed a robust ELT (Extract, Load, Transform) architecture that automates data processing and provides actionable insights. The architecture consists of the following key components:\n\nData Extraction and Loading: Utilized duckdb to efficiently query and merge multiple CSV files from local folders, followed by loading the consolidated data into a PostgreSQL database using the dlt library.\nData Transformation: Implemented dbt to clean, structure, and prepare the data for analysis, following the medallion architecture (bronze, silver, gold layers).\nOrchestration: Employed Dagster to automate and schedule the entire data pipeline, ensuring seamless integration between extraction, transformation, and loading processes.\nVisualization: Leveraged Metabase to create interactive dashboards that provide real-time insights into loan portfolio health, enabling stakeholders to make informed decisions.\nContainerization: Used Docker to containerize the entire data stack, ensuring portability and ease of deployment across different environments.\n\nThe following diagram illustrates the overall ELT architecture employed in this project:\n\n\nELT architecture overview\n\n\n\n\nData Extraction and Loading\nThe first step in our pipeline involved extracting data from local spreadsheet folders (3 source folders were used), including customers folder, location folder, and loan details folder. We utilized an open-source, in-process Online Analytical Processing (OLAP) database management system (duckdb) to query multiple csv files from each folder by merging them using attribute names through efficient analytical duckdb queries. This approach streamlined data extraction (the “E” (Extract) in ELT) , ensuring consistency and reliability across the merged datasets.\nAfter consolidating multiple CSV files, the unified dataset (per folder) was ingested into a PostgreSQL database using the data load tool (dlt). This open-source Python library simplifies data pipelines by automating the “L” (Load) in ELT workflows, making it fast and reliable to handle raw data.\n\n\n\n\n\n\nData Extraction & Loading Implementation Snippet\n\n\n\n\n\n\n#========================================#\n#   Importing required libraries         #\n#========================================#\n\nimport dlt\nimport duckdb\nimport os\nimport logging\nfrom pathlib import Path\nimport dotenv\n\n# ======================================== #\n#           Logging Setup                  #\n# ======================================== #\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s | [%(levelname)s] | %(message)s\",\n)\nlog = logging.getLogger(__name__)\n\n# ======================================== #\n#   Environment Setup (.env for Postgres)  #\n# ======================================== #\ndotenv.load_dotenv()\nconnection_string = os.getenv(\"POSTGRES_CONNECTION_STRING\")\n\nif not connection_string:\n    raise ValueError(\" Missing POSTGRES_CONNECTION_STRING in your .env file!\")\n\npg_destination = dlt.destinations.postgres(connection_string)\n\n# ======================================== #\n#        Directory Setup                   #\n# ======================================== #\nbase_dir = Path(__file__).resolve().parent\ndata_dir = base_dir / \"Data\"\nfact_folder = data_dir / \"fact_db\"\ncustomer_folder = data_dir / \"customer\"\nlocation_folder = data_dir / \"location\"\n\n# Ensure staging truncation\ndlt.config[\"load.truncate_staging_dataset\"] = True\n\n\n# ======================================== #\n#            DLT Resources                 #\n# ======================================== #\n@dlt.resource(table_name=\"loans\", write_disposition=\"replace\")\ndef loans():\n    query = f\"\"\"\n        SELECT *\n        FROM read_csv_auto('{os.path.join(fact_folder, \"*.csv\")}', \n                           union_by_name = true, \n                           filename = true)\n    \"\"\"\n    con = duckdb.connect()\n    con.execute(query)\n    chunk_size = 10000\n    while True:\n        chunk = con.fetch_df_chunk(chunk_size)\n        if chunk is None or chunk.empty:\n            break\n        for record in chunk.to_dict(orient=\"records\"):\n            yield record\n\n\n@dlt.resource(table_name=\"customers\", write_disposition=\"replace\")\ndef customers():\n    query = f\"\"\"\n        SELECT *\n        FROM read_csv_auto('{os.path.join(customer_folder, \"*.csv\")}',\n                           union_by_name = true,\n                           filename = true)\n    \"\"\"\n    con = duckdb.connect()\n    con.execute(query)\n    chunk_size = 10000\n    while True:\n        chunk = con.fetch_df_chunk(chunk_size)\n        if chunk is None or chunk.empty:\n            break\n        for record in chunk.to_dict(orient=\"records\"):\n            yield record\n\n\n@dlt.resource(table_name=\"location\", write_disposition=\"replace\")\ndef locations():\n    query = f\"\"\"\n        SELECT *\n        FROM read_csv_auto('{os.path.join(location_folder, \"*.csv\")}',\n                           union_by_name = true,\n                           filename = true)\n    \"\"\"\n    con = duckdb.connect()\n    con.execute(query)\n    chunk_size = 10000\n    while True:\n        chunk = con.fetch_df_chunk(chunk_size)\n        if chunk is None or chunk.empty:\n            break\n        for record in chunk.to_dict(orient=\"records\"):\n            yield record\n\n\n@dlt.source\ndef source():\n    return [loans(), customers(), locations()]\n\n\n\n\n\n\n\n\n\n\nExpand to explore other file formats DuckDB can directly query using SQL functions\n\n\n\n\n\n\nFile Formats: Parquet,CSV, JSON, Excel (.xlsx files),Text files, Blob files, and ZIP archives.\nIn-Memory Data: Pandas DataFrames, NumPy arrays,Polars DataFrames, and Apache Arrow tables.\nDatabases: PostgreSQL,MySQL, SQLite, and Other DuckDB.\nCloud and Remote Storage: Amazon S3, Google Cloud Storage (GCS), Azure Blob Storage, and HDFS.\nWeb Data: Web APIs and HTTP/HTTPS URLs.\nGeospatial Data: GeoJSON and Shapefiles.\nData Lake Formats: Delta Lake and Iceberg.\n\nRef: DuckDB Documentation\n\n\n\n\n\n\nData Transformation with dbt\nAfter loading, the data was transformed using dbt, an open-source SQL-based tool that lets analysts and engineers clean, structure, and prepare data directly in their warehouse. In ELT workflows, dbt handles the “T” (Transform), turning raw data into analysis-ready datasets.\nUsing dbt allowed modular, reusable transformation logic (See Tip 1). The pipeline followed the medallion architecture where each layer was generated from the previous one, ensuring reliability, reusability, and analytical readiness. The layers included:\n\nBronze (staging): Cleans raw data.\nSilver (intermediate): Standardizes and enriches data.\nGold (marts): Produces aggregated, business-ready insights.\n\n\n\n\n\n\n\ndbt project configuration snippet\n\n\n\n\n\n#=========================================#\n#        dbt_project.yml                  #\n#=========================================#\n\nname: 'postgres_dbt'\nversion: '1.0.0'\nprofile: 'postgres_dbt'\n# Model configurations\nmodels:\n  postgres_dbt:\n    src: \n      +schema: transformed\n      +materialized: ephemeral\n\n    dim:\n      +schema: transformed\n      +materialized: table\n\n    fct:\n      +schema: transformed\n      +materialized: table\n\n    marts:\n      +schema: analytics\n      +materialized: table\n\n\n\n\n1. Source Definition & Staging Layer\n\nPurpose: Acts as the bronze layer, ensuring that raw ingested data tables (declared in sources.yml file) is properly registered, validated, and queryable in dbt.\n\nThis layer defines the source tables ingested into PostgreSQL via dlt, enabling dbt to reference and transform them in subsequent models. The source tables include customer, loans, and location. In this project, we combined source and staging layer to ingest and perform preliminary raw data cleaning from the source tables (customer, loans, location), with minimal cleaning and schema alignment via SQL files.\n\n\n\n\n\n\nNote\n\n\n\nIn a production scenario, the staging layer would typically involve more extensive data validation, cleansing, and transformation to ensure data quality before progressing to the silver layer. Therefore it is recommended to separate source definitions and staging transformations into distinct layers for better maintainability and clarity.\n\n\n\n\n\n\n\n\nSource Definition Snippet\n\n\n\n\n\n\n#========================================#\n#        sources.yml                     #\n#========================================#\n\nsources:\n  - name: portfolio_data\n    schema: staging\n    description: \"Raw data ingested from DLT pipeline containing customers, loans, and location tables.\"\n    tables:\n      - name: customers\n        description: \"Raw customer-level information.\"\n        meta:\n          dagster:\n            asset_key: [\"dlt_source_customers\"]\n\n      - name: loans\n        description: \"Raw loan-level portfolio data.\"\n        meta:\n          dagster:\n            asset_key: [\"dlt_source_loans\"]\n\n      - name: location\n        description: \"Raw location and relationship manager data.\"\n        meta:\n          dagster:\n            asset_key: [\"dlt_source_locations\"]\n\n\n\n\n\n\n\n\n\nSource Table Reference & Staging Layer Implementation Snippet\n\n\n\n\n\n\n--========================================\n-- customer.sql      \n--========================================\nwith customers as (\n    select * \n    from {{ source('portfolio_data', 'customers') }}\n)\n\nselect\n    -- Convert customer ID to lowercase to ensure consistent format\n    TRIM(UPPER(customer_id)) as customer_id,\n    \n    -- Clean and standardize customer name\n    initcap(trim(customer_name)) as customer_name,\n    \n    -- Normalize gender\n    TRIM(UPPER(gender)) as gender,\n\n    -- standardize date of birth\n    CASE\n        WHEN dob::text ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n            THEN dob::date\n        ELSE NULL\n    END AS dob,\n\n    -- Clean and standardize categorical fields\n    trim(initcap(marital_status)) as marital_status,\n    trim(initcap(employment_status)) as employment_status,\n    \n    -- Normalize income bracket\n    trim(income_bracket) as income_bracket,\n    \n    -- Format region names uniformly\n    trim(initcap(region)) as region,\n    \n    -- Cast credit score to integer\n    cast(credit_score as int) as credit_score\n    \nfrom customers\n\n--========================================\n-- location.sql      \n--========================================\n\nwith locations as (\n    select * \n    from {{ source('portfolio_data', 'location') }}\n)\nSELECT\n    TRIM(UPPER(branch_id)) AS branch_id,\n    trim(initcap(branch_name)) AS branch_name,\n    trim(initcap(region)) AS region,\n    trim(initcap(country)) AS country,\n    trim(initcap(manager_name)) AS manager_name,\n    \n    CASE\n        WHEN opened_date::text ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n            THEN opened_date::date\n        ELSE NULL\n    END AS opened_date,\n\n    trim(initcap(branch_type)) AS branch_type\nFROM locations\n\n--========================================\n-- loans.sql      \n--========================================\n\nwith loans as (select * from {{ source('portfolio_data', 'loans') }})\n\nSELECT\n    TRIM(UPPER(loan_id)) AS loan_id,\n    TRIM(UPPER(customer_id)) AS customer_id,\n    TRIM(UPPER(branch_id)) AS branch_id,\n    trim(initcap(loan_product)) AS loan_product,\n\n    CASE\n        WHEN disbursement_date::text ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n            THEN disbursement_date::date\n        ELSE NULL\n    END AS disbursement_date,\n\n    CASE\n        WHEN maturity_date::text ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n            THEN maturity_date::date\n        ELSE NULL\n    END AS maturity_date,\n\n    CAST(loan_amount AS NUMERIC(12,2)) AS loan_amount,\n    CAST(interest_rate AS NUMERIC(5,2)) AS interest_rate,\n    CAST(installment_amount AS NUMERIC(12,2)) AS installment_amount,\n    CAST(outstanding_balance AS NUMERIC(12,2)) AS outstanding_balance,\n    CAST(loan_status AS VARCHAR) AS loan_status,\n    CAST(days_past_due AS NUMERIC(6,2)) AS days_past_due,\n    CAST(amount_overdue AS NUMERIC(12,2)) AS amount_overdue,\n\n    CASE\n        WHEN delinquency_start_date::text ~ '^[0-9]{4}-[0-9]{2}-[0-9]{2}'\n            THEN delinquency_start_date::date\n        ELSE NULL\n    END AS delinquency_start_date,\n\n    CAST(default_flag AS INT) AS default_flag\nFROM loans\n\n\n\n\n\n2. Transformation Layer (Dimensional & Fact Models)\n\nPurpose: Serves as the silver layer, transforming raw data into structured and analysis-ready tables.\n\nThis layer transforms the cleaned staging data into structured, analysis-ready datasets. It includes dimension models (customer and location tables) and a fact model (loan table). The dimension models in this project standardized and enriched customer and location data, while the fact model consolidated loan details with calculated fields for delinquency analysis.\n\n\n\n\n\n\nTransformation Layer Implementation Snippet\n\n\n\n\n\n\n--========================================\n-- Transformation Model: customer.sql        \n--========================================\n\nwith customers as (\n    select *\n    from {{ ref('src_customer') }}\n    where customer_id is not null  -- Exclude invalid records\n      and trim(cast(customer_id as text)) &lt;&gt; ''   -- Exclude empty strings if any\n)\nselect\n    -- create a consistent surrogate key\n    {{ dbt_utils.generate_surrogate_key(['customer_id']) }} as sur_customer_id,\n\n    -- clean and standardized fields\n    customer_id,\n    initcap(trim(customer_name)) as customer_name,\n    case \n        when gender = 'MALE' then 'Male'\n        when gender = 'FEMALE' then 'Female'\n        else 'Unknown'\n    end as gender,\n    dob,\n    marital_status,\n    employment_status,\n    income_bracket,\n    region,\n    credit_score\nfrom customers\n\n--========================================\n-- Transformation Model: location.sql        \n--========================================\n\nwith locations as (\n    select *\n    from {{ ref('src_location') }}\n    where branch_id is not null  -- Exclude invalid records\n      and trim(cast(branch_id as text)) &lt;&gt; ''   -- Exclude empty strings if any\n)\n\nselect\n    -- create a consistent surrogate key\n    {{ dbt_utils.generate_surrogate_key(['branch_id']) }} as branch_id_sur,\n\n    -- original fields\n    branch_id,\n    branch_name,\n    region,\n    country,\n    manager_name,\n    opened_date,\n    branch_type\nfrom locations\n\n--========================================\n-- Transformation Model: loans.sql        \n--========================================\n\nwith loans as (\n    select *\n    from {{ ref('src_loans') }}\n      where loan_id is not null  -- Exclude invalid records\n      and trim(cast(loan_id as text)) &lt;&gt; ''   -- Exclude empty strings if any\n)\nselect\n    -- create consistent surrogate keys\n    {{ dbt_utils.generate_surrogate_key(['loan_id']) }} as sur_loan_id,\n    {{ dbt_utils.generate_surrogate_key(['customer_id']) }} as sur_customer_id,\n    {{ dbt_utils.generate_surrogate_key(['branch_id']) }} as sur_branch_id,\n\n    -- attributes\n    loan_id,\n    customer_id,\n    branch_id,\n    loan_product,\n    disbursement_date,\n    maturity_date,\n    loan_amount,\n    interest_rate,\n    installment_amount,\n    outstanding_balance,\n    loan_status,\n    days_past_due as days_late,\n    CASE WHEN days_past_due &lt; 1 THEN days_past_due ELSE NULL END AS PAR_0,\n    CASE WHEN days_past_due &gt;= 1 AND days_past_due &lt; 30 THEN days_past_due ELSE NULL END AS PAR_30,\n    CASE WHEN days_past_due &gt;= 30 AND days_past_due &lt; 60 THEN days_past_due ELSE NULL END AS PAR_60,\n    CASE WHEN days_past_due &gt;= 60 THEN days_past_due ELSE NULL END AS PAR_90,\n    amount_overdue,\n    delinquency_start_date,\n    default_flag\nfrom loans\n\n\n\n\n\n3. Marts Layer\n\nPurpose: Acts as the gold layer, delivering curated datasets for analytics and reporting.\n\nThis project utilized this layer to create business-facing datasets for analytics and machine learning data models. It consolidated dimension and fact models into an analytics table that supported Metabase dashboards and generated feature tables for ML models predicting loan defaults (future work).\n\n\n\n\n\n\nMarts Layer Implementation Snippet\n\n\n\n\n\n\n--========================================\n-- Analytics Model: Analytics.sql        \n--========================================\nWITH \n    -- Dimension tables\n    dim_customers AS (\n        SELECT\n            customer_id,\n            customer_name,\n            gender,\n            dob,\n            marital_status,\n            employment_status,\n            income_bracket\n        FROM {{ ref('customers') }}\n    ),\n\n    dim_location AS (\n        SELECT\n            branch_id,\n            branch_name,\n            region,\n            country,\n            manager_name,\n            opened_date,\n            branch_type\n        FROM {{ ref('location') }}\n    ),\n\n    -- Fact table (transactional data)\n    fact_loans AS (\n        SELECT\n            loan_id, \n            customer_id, \n            branch_id,             \n            loan_product,\n            disbursement_date,\n            maturity_date,\n            loan_amount AS disbursed_amt,\n            outstanding_balance AS outstanding_bal,\n            loan_status,\n            interest_rate,\n            days_late,\n            par_0,\n            par_30,\n            par_60,\n            par_90,\n            amount_overdue,\n            delinquency_start_date\n        FROM {{ ref('fact_tables') }}\n    )\n\n-- === STAR JOIN ===\nSELECT \n    -- Key alignment \n    f.customer_id AS fact_customer_id,\n    c.customer_id AS dim_customer_id,\n    f.branch_id AS fact_branch_id,\n    o.branch_id AS dim_branch_id,\n    f.loan_id,\n\n    -- Customer attributes\n    c.customer_name,\n    c.gender,\n    c.marital_status,\n    c.employment_status,\n    c.income_bracket,\n\n    -- Location attributes\n    o.branch_name,\n    o.region,\n    o.country,\n    o.manager_name,\n    o.branch_type,\n\n    -- Loan attributes\n    f.loan_product,\n    f.disbursement_date,\n    f.maturity_date,\n    f.disbursed_amt,\n    f.outstanding_bal,\n    f.interest_rate,\n    f.loan_status,\n    f.days_late,\n    f.par_0,\n    f.par_30,\n    f.par_60,\n    f.par_90,\n    f.amount_overdue,\n    f.delinquency_start_date,\n\n    -- Loan status flag\n    CASE \n        WHEN f.days_late &gt;= 60 THEN 1\n        ELSE 0\n    END AS default_status,\n\n    -- Portfolio at Risk (PAR) ratio\n    ROUND(\n        COALESCE(f.par_30 + f.par_60 + f.par_90, 0)::NUMERIC \n        / NULLIF(f.outstanding_bal, 0), \n        4\n    ) AS portfolio_at_risk_ratio,\n\n    -- Derived time dimensions\n    EXTRACT(YEAR FROM f.disbursement_date) AS disbursement_year,\n    EXTRACT(MONTH FROM f.disbursement_date) AS disbursement_month,\n\n    -- Ranking by disbursement period\n    DENSE_RANK() OVER (\n        ORDER BY \n            EXTRACT(YEAR FROM f.disbursement_date),\n            EXTRACT(MONTH FROM f.disbursement_date)\n    ) AS disbursement_month_rank\n\nFROM fact_loans f\nINNER JOIN dim_customers c \n    ON f.customer_id = c.customer_id\nINNER JOIN dim_location o \n    ON f.branch_id = o.branch_id\n\n--========================================\n-- ML Model: training.sql        \n--========================================\n\n{% set train_start = '2022-01-01' %}\n{% set train_end = '2024-12-31' %}\n{% set observation_window_months = 6 %}\n\nWITH training AS (\n    SELECT\n        --ID columns \n        f.fact_customer_id,\n        f.dim_customer_id,\n        f.fact_branch_id,\n        f.dim_branch_id,\n\n        -- Loan details\n        f.loan_id,\n        f.loan_product,\n        f.disbursement_date,\n        f.disbursed_amt,\n        f.outstanding_bal,\n        f.interest_rate,\n\n        -- Customer attributes\n        f.gender,\n        f.marital_status,\n        f.employment_status,\n        f.income_bracket,\n\n        -- Branch attributes\n        f.region,\n        f.branch_type,\n\n        -- Loan performance metrics\n        f.days_late,\n        f.par_0,\n        f.par_30,\n        f.par_60,\n        f.par_90,\n\n        -- Target variable (label)\n        f.default_status AS target_default,\n\n        -- Derived date fields\n        f.disbursement_month,\n        f.disbursement_year\n\n    FROM {{ ref('analytics') }} AS f\n    WHERE\n        f.disbursement_date BETWEEN '{{ train_start }}' AND '{{ train_end }}'\n        -- Ensure at least full observation period before today\n        -- We only want to include loans that have had enough time to observe default behavior\n        AND f.disbursement_date &lt;= CURRENT_DATE - INTERVAL '{{ observation_window_months }} months'\n)\n\nSELECT *\nFROM training\n\n--========================================\n-- ML Model: testing.sql        \n--========================================\n-- Testing/Evaluation set: loans disbursed from 2025-01-01 to 2025-10-01\n-- Must have full 6-month observation window\n\n{% set test_start = '2025-01-01' %}\n{% set test_end = '2025-10-01' %}\n{% set observation_window_months = 6 %}\n\nWITH testing AS (\n    SELECT\n        -- ID columns \n        f.fact_customer_id,\n        f.dim_customer_id,\n        f.fact_branch_id,\n        f.dim_branch_id,\n\n        -- Core loan attributes\n        f.loan_id,\n        f.loan_product,\n        f.disbursement_date,\n        f.disbursed_amt,\n        f.outstanding_bal,\n        f.interest_rate,\n\n        -- Customer attributes\n        f.gender,\n        f.marital_status,\n        f.employment_status,\n        f.income_bracket,\n\n        -- Branch attributes\n        f.region,\n        f.branch_type,\n\n        -- Performance metrics and target label\n        f.days_late,\n        f.par_0,\n        f.par_30,\n        f.par_60,\n        f.par_90,\n        f.default_status AS target_default,\n\n        --  Derived date fields\n        f.disbursement_month,\n        f.disbursement_year\n\n    FROM {{ ref('analytics') }} AS f\n    WHERE \n        f.disbursement_date BETWEEN '{{ test_start }}' AND '{{ test_end }}'\n        -- Ensure full observation period (at least 6 months since disbursement)\n        AND f.disbursement_date &lt;= CURRENT_DATE - INTERVAL '{{ observation_window_months }} months'\n)\n\nSELECT *\nFROM testing\n\n\n--========================================\n-- ML Model: inference.sql        \n--========================================\n-- Inference set: NEW loans disbursed in November 2025\n-- These loans have no target label (yet)\n-- Only include features known at disbursement\n\n{% set inference_month_start = '2025-11-01' %}\n{% set inference_month_end = '2025-11-30' %}\n\nWITH inference AS (\n    SELECT\n        -- ID columns\n        f.fact_customer_id,\n        f.dim_customer_id,\n        f.fact_branch_id,\n        f.dim_branch_id,\n\n        -- Loan details\n        f.loan_id,\n        f.loan_product,\n        f.disbursement_date,\n        f.disbursed_amt,\n        f.outstanding_bal,\n        f.interest_rate,\n\n        --  Customer attributes\n        f.gender,\n        f.marital_status,\n        f.employment_status,\n        f.income_bracket,\n\n        -- Location attributes\n        f.region,\n        f.branch_type,\n\n        -- Derived date columns\n        f.disbursement_month,\n        f.disbursement_year\n\n    FROM {{ ref('analytics') }} AS f\n    WHERE\n        f.disbursement_date BETWEEN '{{ inference_month_start }}' AND '{{ inference_month_end }}'\n        -- Exclude any loans already defaulted (we only want active/new)\n        AND f.default_status = 0\n)\n\nSELECT *\nFROM inference\n\n\n\n\n\n\n\n\n\nTip 1: SQL Templating with Jinja\n\n\n\nJinja templating throughout the models ensured dynamic SQL, avoiding code duplication, and guaranteed scalability and consistency across the pipeline.\n\n\n\n\n\n\nOrchestration with Dagster\nTo automate the entire data pipeline, we employed Dagster, a powerful orchestration tool. Dagster enabled the definition and scheduling of workflows that encompass data extraction, transformation, and loading processes. By setting up Dagster jobs ensured that the pipeline runs seamlessly and consistently, providing up-to-date insights into the loan portfolio. The figure below illustrates the Dagster pipeline architecture used in this project.\nDagster Pipeline Visualization:\n\n\n\n\n\n\n\nPipeline Orchestration with Dagster Snippet\n\n\n\n\n\n\n# ==================== #\n#       Imports        #\n# ==================== #\n\nfrom pathlib import Path\nimport sys\nimport os\nimport logging\nfrom dotenv import load_dotenv\nimport dlt\nimport dagster as dg\nfrom dagster_dlt import DagsterDltResource, dlt_assets\nfrom dagster_dbt import DbtCliResource, dbt_assets, DbtProject\n\n\n# ==================== #\n#    Logging Setup     #\n# ==================== #\nBASE_DIR = Path(__file__).parents[1]\nLOGS_DIR = BASE_DIR / \"logs\"\nLOGS_DIR.mkdir(exist_ok=True)\n\nlog_file = LOGS_DIR / \"dagster_orchestration.log\"\n\nlogging.basicConfig(\n    filename=log_file,\n    level=logging.INFO,\n    format=\"%(asctime)s | [%(levelname)s] | %(message)s\",\n)\n\nconsole = logging.StreamHandler()\nconsole.setLevel(logging.INFO)\nconsole.setFormatter(logging.Formatter(\"%(asctime)s | [%(levelname)s] | %(message)s\"))\nlogging.getLogger().addHandler(console)\n\nlogging.info(\" Starting Dagster orchestration setup...\")\n\n\n# ==================== #\n#        Setup         #\n# ==================== #\n\n# Add ETL base directory to sys.path for imports\nsys.path.insert(0, str(BASE_DIR))\nfrom data_extraction.load_data import source  # Import the source function\nlogging.info(\" Imported DLT source from data_extraction.load_data\")\n\n# Load environment variables from .env file\nENV_PATH = BASE_DIR / \".env\"\nload_dotenv(dotenv_path=ENV_PATH)\n\nconnection_string = os.getenv(\"POSTGRES_CONNECTION_STRING\")\nif not connection_string:\n    logging.error(\" Missing POSTGRES_CONNECTION_STRING in .env file\")\n    raise ValueError(\"Missing POSTGRES_CONNECTION_STRING in .env\")\n\nlogging.info(f\" Using PostgreSQL connection: {connection_string}\")\n\n# Initialize Dagster DLT resource\ndlt_resource = DagsterDltResource()\n\n\n# ==================== #\n#       DLT ASSET      #\n# ==================== #\n@dlt_assets(\n    dlt_source=source(),  \n    dlt_pipeline=dlt.pipeline(\n        pipeline_name=\"loans_pipeline\",\n        dataset_name=\"staging\",\n        destination=dlt.destinations.postgres(connection_string)\n    ),\n)\ndef dlt_load(context: dg.AssetExecutionContext, dlt: DagsterDltResource):\n    \"\"\"Load local CSVs (customers, loans, location) into PostgreSQL via DLT pipeline.\"\"\"\n    logging.info(\" [DLT] Starting pipeline execution through Dagster...\")\n    try:\n        yield from dlt.run(context)  # Execute the DLT pipeline\n        logging.info(\" [DLT] Pipeline completed successfully via Dagster.\")\n    except Exception as e:\n        logging.exception(f\" [DLT] Pipeline execution failed: {e}\")\n        raise\n\n\n# ==================== #\n#       DBT ASSET      #\n# ==================== #\ndbt_project_dir = BASE_DIR / \"postgres_dbt\"\nprofiles_dir = dbt_project_dir  # profiles.yml is here\n\ndbt_project = DbtProject(project_dir=dbt_project_dir, profiles_dir=profiles_dir)\ndbt_resource = DbtCliResource(project_dir=dbt_project_dir, profiles_dir=profiles_dir)\n\ndbt_project.prepare_if_dev()\nlogging.info(\" [DBT] Project and profiles initialized successfully\")\n\n@dbt_assets(manifest=dbt_project.manifest_path)\ndef dbt_models(context: dg.AssetExecutionContext, dbt: DbtCliResource):\n    \"\"\"Run and expose all dbt models (staging → transformed → marts).\"\"\"\n    logging.info(\" [DBT] Running dbt build...\")\n    try:\n        yield from dbt.cli([\"build\"], context=context).stream()\n        logging.info(\" [DBT] dbt build completed successfully.\")\n    except Exception as e:\n        logging.exception(f\" [DBT] dbt build failed: {e}\")\n        raise\n\n\n# ==================== #\n#         JOBS         #\n# ==================== #\n\n# Define a job for DLT loading\njob_dlt = dg.define_asset_job(\n    \"job_dlt\",\n    selection=dg.AssetSelection.keys(\n        \"dlt_source_loans\",      # These are the actual asset keys created by @dlt_assets\n        \"dlt_source_customers\", \n        \"dlt_source_locations\"\n    ),\n)\n\n# Define a job for dbt transformations\njob_dbt = dg.define_asset_job(\n    \"job_dbt\",\n    selection=dg.AssetSelection.key_prefixes(\"staging\", \"transformed\", \"marts\")\n)\n\n\n# ==================== #\n#       SCHEDULES      #\n# ==================== #\nschedule_dlt = dg.ScheduleDefinition(\n    job=job_dlt,\n    cron_schedule=\"10 1 * * *\"  # 01:10 UTC = 04:10 Nairobi\n)\nlogging.info(\"Dagster schedule created for job_dlt (daily 04:10 Nairobi)\")\n\n\n# ==================== #\n#       SENSORS        #\n# ==================== #\n@dg.multi_asset_sensor(\n    monitored_assets=[\n        dg.AssetKey(\"dlt_source_customers\"),\n        dg.AssetKey(\"dlt_source_loans\"),\n        dg.AssetKey(\"dlt_source_locations\"),\n    ],\n    job=job_dbt,\n)\ndef dlt_load_sensor(context):\n    \"\"\"Trigger dbt job after all DLT ingestion assets complete.\"\"\"\n    logging.info(\"⚡ Sensor triggered: DLT assets finished loading. Launching dbt job...\")\n    yield dg.RunRequest(run_key=None)\n\n\n# ==================== #\n#      DEFINITIONS     #\n# ==================== #\ndefs = dg.Definitions(\n    assets=[dlt_load, dbt_models],\n    resources={\"dlt\": dlt_resource, \"dbt\": dbt_resource},\n    jobs=[job_dlt, job_dbt],\n    schedules=[schedule_dlt],\n    sensors=[dlt_load_sensor]\n)\n\nlogging.info(\" Dagster definitions initialized successfully.\")\nlogging.info(\" Dagster orchestration setup complete — ready for `dagster dev`.\")\n\n\n\n\n\n\n\nDockerization\nTo ensure that our data stack is portable and easy to deploy, we containerized each component using Docker. This approach allowed us to package the PostgreSQL database, dbt transformations, Dagster orchestration, and Metabase visualization into containers. Docker Compose was employed to manage multi-container applications, allowing us to define and run the entire data stack with a single command. This setup not only simplified deployment but also enhanced scalability and maintainability.\n\n\n\n\n\n\nDocker Compose Snippet\n\n\n\n\n\n#========================================#\n#        docker-compose.yml               #\n#========================================#\nservices:\n  dwh_pipeline:\n    build:\n      context: .\n      dockerfile: Dockerfile.dwh\n    container_name: dwh_pipeline\n    env_file:\n      - .env\n    environment:\n      CREDENTIALS__CONNECTION_STRING: ${POSTGRES_CONNECTION_STRING}\n      DBT_HOST: ${DBT_HOST}\n      DBT_USER: ${DBT_USER}\n      DBT_PASSWORD: ${DBT_PASSWORD}\n      DBT_PORT: ${DBT_PORT}\n      DBT_DBNAME: ${DBT_DBNAME}\n      DBT_SCHEMA: ${DBT_SCHEMA}\n    volumes:\n      # Existing data mount\n      - ./data_extraction/Data:/pipeline/data_extraction/Data\n\n      # Additional volumes for runtime editing and live data\n      - ./data_extraction:/pipeline/data_extraction\n      - ./postgres_dbt:/pipeline/postgres_dbt\n      - ./orchestration:/pipeline/orchestration\n\n      # Mount dbt profile directly \n      - ./postgres_dbt/profiles.yml:/root/.dbt/profiles.yml\n\n      # Mount global data directory \n      - ./data:/pipeline/data\n\n      # Mount requirements file for live dependency changes\n      - ./requirements.txt:/pipeline/orchestration/requirements.txt\n\n      # Mount .env file for consistent runtime environment\n      - ./.env:/pipeline/.env\n\n    expose:\n      - 3001\n    depends_on:\n      - postgres_etl\n    networks:\n      - dokploy-network\n    labels:\n      # Network labels are deliberately removed for security purposes\n\n  postgres_etl:\n    image: postgres:latest\n    container_name: postgres_etl\n    environment:\n      POSTGRES_USER: ${DBT_USER}\n      POSTGRES_DB: ${DBT_DBNAME}\n      POSTGRES_PASSWORD: ${DBT_PASSWORD}\n    volumes:\n      - etl_pgdata:/var/lib/postgresql/data\n    networks:\n      - dokploy-network\n\nvolumes:\n  etl_pgdata:\n\nnetworks:\n  dokploy-network:\n    external: true\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nContact to access the deployed warehouse pipeline.\n\n\n\n\n\nConclusion\nThis project demonstrates the effectiveness of a modern data stack in managing loan portfolios for financial institutions. By integrating PostgreSQL, dbt, Dagster, and Metabase, we created an automated pipeline that enhances data management, transformation, and visualization. The solution provides real-time insights into portfolio health, enabling better risk assessment and decision-making. Future work will focus on incorporating machine learning models to predict borrower defaults to further enhance portfolio management strategies.\n\n\n\nFuture Work\nFuture enhancements to this project could include the integration of machine learning algorithms to predict loan defaults based on historical data. Additionally, expanding the data sources to include alternative credit data and social media insights could provide a more comprehensive view of borrower risk. Implementing advanced analytics and reporting features in Metabase would also enhance the user experience and provide deeper insights into portfolio performance.\n\n\n\nAcknowledgments\nI would like to express my gratitude to the open-source community for providing the tools and resources that made this project possible. Special thanks to the developers of PostgreSQL, dbt, Dagster, and Metabase for their continuous efforts in advancing data management and analytics.\n\n\n\nReferences\n\ndlt\nduckdb\ndbt\ndagster\nmetabase"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Mulimbika Makina",
    "section": "",
    "text": "Get in touch\n          Let's build something amazing together!\n        \n\n        \n        \n          \n          \n          \n            Don’t fill this out if you're human: \n          \n\n          \n            \n            Full Name\n          \n\n          \n            \n            Email Address\n          \n\n          \n            \n            Phone Number (Optional)\n          \n\n          \n            \n            Your Message\n          \n\n          \n            \n              Send Message"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n    From Data to Impact\n  ",
    "section": "",
    "text": "Data Science · Business Intelligence\n  \n\n  \n  \n    From Data to Impact\n  \n\n  \n  \n    I’m Mulimbika Makina, a Data Scientist specializing in mathematical and statistical rigor \n    to drive strategic business decisions through complex data analysis.\n  \n\n  \n  \n    Python\n    SQL\n    R\n    Deep Learning (PyTorch)\n    LLMs\n    NLP\n    Power BI\n    Machine Learning\n    Statistics\n    Metabase\n  \n\n  \n  \n    \n      Explore Projects\n    \n    \n      Review CV"
  },
  {
    "objectID": "posts/Machine Learning End to End/index.html#model-training-workflow",
    "href": "posts/Machine Learning End to End/index.html#model-training-workflow",
    "title": "End to End Machine Learning with Deployment",
    "section": "7.1 Model Training Workflow",
    "text": "7.1 Model Training Workflow\nThe following flowchart presents a simplified view of the model training pipeline used in this project.\nIt focuses on the core stages from data preparation to model persistence, abstracting away implementation details.\n\n\n\n\n\nflowchart TD\n\n    A([Start Model Training]) --&gt; B[Load Pre-split Transformed Data]\n\n    B --&gt; C[Separate Features and Target]\n\n    C --&gt; D[Define Candidate Models]\n\n    D --&gt; E[Train and Evaluate Models]\n\n    E --&gt; F[Select Best Performing Model]\n\n    F --&gt; G[Evaluate on Train and Test Data]\n\n    G --&gt; H{Overfitting Check}\n\n    H --&gt;|Detected| I[Log Warning]\n    H --&gt;|Not Detected| J[Proceed]\n\n    I --&gt; J\n\n    J --&gt; K[Log Metrics and Model with MLflow]\n\n    K --&gt; L[Save Model and Artifacts]\n\n    L --&gt; M([End])"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Mulimbika Makina",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nEnd to End Machine Learning with Deployment\n\n\nPredicting Customer Churn\n\n\n\n\n\n\n\n\nDec 12, 2025\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\nHybrid RAG-SQL Agent for Enterprise Knowledge Systems\n\n\nReliable Question Answering over Unstructured and Structured Enterprise Data\n\n\n\n\n\n\n\n\nFeb 4, 2026\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nModern Loan Portfolio Data Stack for Financial Institutions\n\n\nImproving portfolio health and minimizing loan defaults through intelligent automated data pipelines and real-time insights.\n\n\n\n\n\n\n\n\nNov 13, 2025\n\n\n33 min\n\n\n\n\n\n\nNo matching items"
  }
]